# ir_system/services/document_service.py

import re
import html
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import os
import pandas as pd
import ir_datasets
import string
from symspellpy import SymSpell, Verbosity
import pkg_resources

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

# Ø¥Ø¹Ø¯Ø§Ø¯ SymSpell Ù„Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
dictionary_path = pkg_resources.resource_filename(
    "symspellpy", "frequency_dictionary_en_82_765.txt"
)
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)


def semantic_spell_check(text):
    """
    ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù…Ø¹Ù†Ù‰
    """
    words = text.split()
    corrected_words = []
    
    for word in words:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
        
        if suggestions:
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ù‚Ø±Ø¨
            corrected_words.append(suggestions[0].term)
        else:
            corrected_words.append(word)
    
    return ' '.join(corrected_words)

def safe_text(text):
    """
    ÙŠØ­Ø§ÙˆÙ„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø®Ø§Ø·Ø¦ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.
    """
    try:
        return text.encode('latin1').decode('utf-8')
    except:
        return text

def query_advanced_preprocess(text):
    """
    ØªÙ†Ø¸ÙŠÙ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†ØµÙˆØµ ÙŠØ´Ù…Ù„:
    - Ø¥Ø²Ø§Ù„Ø© HTML ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    - ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
    - ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø¯Ù„Ø§Ù„ÙŠØ§Ù‹
    - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆØ§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    - Stemming
    """
    text = html.unescape(text)
    text = ''.join(c for c in text if c.isprintable())
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-z\s]', ' ', text)
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # # ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©
    # text = semantic_spell_check(text)
    
    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]
    return ' '.join(words)

punctuation_to_remove = string.punctuation.replace("?", "")
punctuation_regex = re.compile(f"[{re.escape(punctuation_to_remove)}]")

def preprocess(text):
    # 1. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©
    text = text.lower()
    
    # 2. Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… (Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ "?")
    text = punctuation_regex.sub("", text)
    
    # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = re.sub(r"\s+", " ", text).strip()
    
    # 4. Tokenization
    tokens = text.split()
    
    # 5. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    tokens = [token for token in tokens if len(token) >= 2]
    
    return tokens 

def advanced_preprocess(text):
    # Lowercase
    text = text.lower()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª
    text = re.sub(r'\S+@\S+', '', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML
    text = re.sub(r'<.*?>', '', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    text = re.sub(r'\d+', '', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
    text = re.sub(r'[^a-z\s]', ' ', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø­Ø±ÙˆÙ
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenization + Stopword Removal + Stemming
    tokens = text.split()
    processed = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]

    return processed


def process_documents(limit=200000, dataset_name="msmarco-passage/train", output_file=None):
    dataset = ir_datasets.load(dataset_name)
    doc_iterator = dataset.docs_iter()
    docs_data = []
    count = 0

    while count < limit:
        try:
            doc = next(doc_iterator)
            text_clean = safe_text(doc.text)
            docs_data.append({
                "id": doc.doc_id,
                "text": text_clean
            })
            count += 1
            if count % 10000 == 0:
                print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...")
        except Exception:
            continue

    df = pd.DataFrame(docs_data)
    print("\nğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...")
    df['clean_text'] = df['text'].apply(advanced_preprocess)

    # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¥Ø°Ø§ Ù…Ø§ Ø§Ù†Ù…Ø±Ø±
    if output_file is None:
        output_dir = os.path.join(os.path.dirname(__file__), "..", "data", "msmarco_train", "processed")
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "processed_msmarco_train.json")
        