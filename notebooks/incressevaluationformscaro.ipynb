{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5208eab9-7534-4e5b-88f2-76950a4aa1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 380000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 390000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 400000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 410000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 420000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 430000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 440000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 450000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 460000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 470000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 480000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 490000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 500000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "\n",
      "ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Tokens...\n",
      "\n",
      "ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\n",
      "..\\data\\msmarco_train_test\\processed\\tokenized_msmarco.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "# 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø©\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 500_000\n",
    "seen_ids = set()  # Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±\n",
    "\n",
    "# 3. ØªÙ†Ø¸ÙŠÙ Unicode Ø§Ù„Ù…Ø´ÙˆÙ‡\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        text = html.unescape(text)  # Ø¥Ø²Ø§Ù„Ø© HTML entities Ù…Ø«Ù„ &amp;\n",
    "        text = unicodedata.normalize(\"NFKD\", text)  # Normalize Unicode\n",
    "        text = ''.join(c for c in text if c.isprintable())  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ©\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# 5. Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© â€“ ØªÙØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª (tokens)\n",
    "def advanced_preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù…\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø²Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø­Ø±ÙˆÙ\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenization + Stopword Removal + Stemming\n",
    "    tokens = text.split()\n",
    "    processed = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return processed\n",
    "\n",
    "# 6. ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        clean_original = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": clean_original\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# 7. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 8. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… => tokens\n",
    "print(\"\\nğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Tokens...\")\n",
    "df['tokens'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 9. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø­Ø³Ø¨ ID\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 10. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ JSON\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\processed\"\n",
    "output_file = os.path.join(output_dir, \"tokenized_msmarco.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'tokens']].to_json(output_file, orient='records', lines=False, force_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51043531-28ab-4d95-89b0-b8abf8b11f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import advanced_preprocess\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "input_file = r\"..\\data\\msmarco_train_test\\processed\\tokenized_msmarco.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ§Øª\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [\" \".join(doc['tokens']) for doc in data]  # Ù†ØµÙˆØµ Ù†ØµÙŠØ©\n",
    "\n",
    "# 3. Ø¥Ø¹Ø¯Ø§Ø¯ TfidfVectorizer Ù…Ø¹ preprocess ÙƒÙ€ tokenizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=advanced_preprocess,   # Ø¯Ø§Ù„Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "    preprocessor=None,      # Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… preprocessor Ø¢Ø®Ø±\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    lowercase=False         # Ù„Ø£Ù† preprocess ØªÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø¥Ù„Ù‰ ØµØºÙŠØ±Ø© Ø¨Ø§Ù„ÙØ¹Ù„\n",
    ")\n",
    "\n",
    "# 4. ØªØ·Ø¨ÙŠÙ‚ TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_msmarco_train_test.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_msmarco_train_test.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_msmarco_train_test.joblib\"))\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02380e3d-bd82-4470-b1e2-39a17551f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ---\n",
      "Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯: ..\\data\\msmarco_train_test\\index\\TFIDF\n",
      "\n",
      "--- 2. Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ---\n",
      "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­.\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 500000\n",
      "Ø£Ø¨Ø¹Ø§Ø¯ Ù…ØµÙÙˆÙØ© TF-IDF: (500000, 91371)\n",
      "\n",
      "--- 3. Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ---\n",
      "Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ù…ØµÙÙˆÙØ© TF-IDF (Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f503df2c23473b89019da9297d0d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³:   0%|          | 0/11624617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¬Ø§Ø±ÙŠ ÙØ±Ø² Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„ÙƒÙ„ Ù…ØµØ·Ù„Ø­ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© TF-IDF...\n",
      "ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ø¨Ù†Ø¬Ø§Ø­.\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: 91371\n",
      "\n",
      "--- 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ ---\n",
      "\n",
      "--- 5. Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ---\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: ..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_inverted_index.joblib\n",
      "\n",
      "--- 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ÙÙˆØ¸ ---\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ù„Ù„ØªØ­Ù‚Ù‚.\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³: 91371\n",
      "Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ('presenc'): [('409792', 0.4636566116093226), ('580660', 0.44391864539155124), ('774505', 0.43167140720627645), ('569503', 0.41676428533006127), ('585454', 0.3902043674130837)]\n",
      "\n",
      "--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø¨Ù†Ø¬Ø§Ø­ ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ------------------\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import advanced_preprocess\n",
    "\n",
    "# ------------------ 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ------------------\n",
    "print(\"--- 1. ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ---\")\n",
    "doc_ids_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\doc_ids_msmarco_train_test.joblib\"\n",
    "tfidf_matrix_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_matrix_msmarco_train_test.joblib\"\n",
    "tfidf_vectorizer_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_vectorizer_msmarco_train_test.joblib\"\n",
    "\n",
    "output_inverted_index_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_inverted_index.joblib\"\n",
    "\n",
    "output_dir = os.path.dirname(output_inverted_index_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯: {output_dir}\")\n",
    "\n",
    "# ------------------ 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ------------------\n",
    "print(\"\\n--- 2. Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ---\")\n",
    "try:\n",
    "    doc_ids = joblib.load(doc_ids_path)\n",
    "    tfidf_matrix = joblib.load(tfidf_matrix_path)\n",
    "    tfidf_vectorizer = joblib.load(tfidf_vectorizer_path)\n",
    "    print(\"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­.\")\n",
    "    print(f\"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(doc_ids)}\")\n",
    "    print(f\"Ø£Ø¨Ø¹Ø§Ø¯ Ù…ØµÙÙˆÙØ© TF-IDF: {tfidf_matrix.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø­Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª.\")\n",
    "    print(f\"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙÙ‚ÙˆØ¯: {e.filename}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ------------------ 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ------------------\n",
    "print(\"\\n--- 3. Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ---\")\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "inverted_index = {}\n",
    "\n",
    "tfidf_matrix_coo = tfidf_matrix.tocoo()\n",
    "print(\"Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ù…ØµÙÙˆÙØ© TF-IDF (Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª)...\")\n",
    "\n",
    "for doc_idx, term_idx, tfidf_score in tqdm(zip(tfidf_matrix_coo.row, tfidf_matrix_coo.col, tfidf_matrix_coo.data),\n",
    "                                           total=len(tfidf_matrix_coo.data),\n",
    "                                           desc=\"Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³\"):\n",
    "    term = feature_names[term_idx]\n",
    "    doc_id = doc_ids[doc_idx]\n",
    "    if term not in inverted_index:\n",
    "        inverted_index[term] = []\n",
    "    inverted_index[term].append((doc_id, float(tfidf_score)))\n",
    "\n",
    "print(\"Ø¬Ø§Ø±ÙŠ ÙØ±Ø² Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„ÙƒÙ„ Ù…ØµØ·Ù„Ø­ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© TF-IDF...\")\n",
    "for term in inverted_index:\n",
    "    inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ø¨Ù†Ø¬Ø§Ø­.\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {len(inverted_index)}\")\n",
    "\n",
    "# ------------------ 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ ------------------\n",
    "print(\"\\n--- 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ ---\")\n",
    "inverted_index_data = {\n",
    "    \"inverted_index\": inverted_index,\n",
    "    \"num_documents\": len(doc_ids),\n",
    "    \"num_terms\": len(inverted_index),\n",
    "    \"vocabulary_size\": len(feature_names),\n",
    "    \"vectorizer_vocabulary\": dict(tfidf_vectorizer.vocabulary_)\n",
    "}\n",
    "\n",
    "# ------------------ 5. Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ ------------------\n",
    "print(\"\\n--- 5. Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ---\")\n",
    "try:\n",
    "    joblib.dump(inverted_index_data, output_inverted_index_path)\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {output_inverted_index_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³: {e}\")\n",
    "\n",
    "# ------------------ 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­ÙØ¸ ------------------\n",
    "print(\"\\n--- 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ÙÙˆØ¸ ---\")\n",
    "try:\n",
    "    loaded_data = joblib.load(output_inverted_index_path)\n",
    "    loaded_index = loaded_data[\"inverted_index\"]\n",
    "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ù„Ù„ØªØ­Ù‚Ù‚.\")\n",
    "    print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³: {len(loaded_index)}\")\n",
    "\n",
    "    example_term = next(iter(loaded_index))\n",
    "    print(f\"Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ ('{example_term}'): {loaded_index[example_term][:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚: {e}\")\n",
    "\n",
    "print(\"\\n--- âœ… Ø§Ù†ØªÙ‡Ù‰ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø¨Ù†Ø¬Ø§Ø­ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13985db-aed3-4f1d-9e81-8b3069632404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 380000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 390000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 400000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 410000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 420000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 430000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 440000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 450000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 460000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 470000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 480000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 490000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 500000 ÙˆØ«ÙŠÙ‚Ø©...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "seen_ids = set()\n",
    "\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "count = 0\n",
    "\n",
    "docs_data = []\n",
    "\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "while count < 500000:\n",
    "\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "df.drop(df.loc[df['id']==''].index,inplace=True)\n",
    "\n",
    "df['id']=df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adafee3-aabe-4728-8f4f-b6f99776bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\raw\"\n",
    "output_file = os.path.join(output_dir, \"raw_msmarco_train_test.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_json(output_file, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f496196-e4c3-4596-8a4f-07461622bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TFIDF:   0%|                                                                 | 5/10000 [00:00<04:23, 37.90it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.65s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 16/10000 [00:02<25:05,  6.63it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.72s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 17/10000 [00:02<32:09,  5.17it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 18/10000 [00:03<46:47,  3.56it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.66s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 19/10000 [00:03<54:49,  3.03it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|â–                                                               | 49/10000 [00:09<44:57,  3.69it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.62s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–                                                               | 66/10000 [00:11<20:42,  8.00it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.53s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–                                                               | 69/10000 [00:12<33:46,  4.90it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–                                                               | 73/10000 [00:13<26:13,  6.31it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–Œ                                                               | 84/10000 [00:15<20:04,  8.23it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–Œ                                                               | 86/10000 [00:15<32:51,  5.03it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.61s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–Œ                                                               | 93/10000 [00:17<42:55,  3.85it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–‰                                                              | 146/10000 [00:27<36:25,  4.51it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.87s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|â–‰                                                              | 149/10000 [00:27<36:35,  4.49it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|â–ˆ                                                              | 178/10000 [00:34<41:13,  3.97it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.52s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|â–ˆâ–                                                             | 193/10000 [00:36<19:31,  8.37it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.60s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|â–ˆâ–                                                             | 199/10000 [00:37<41:36,  3.93it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   3%|â–ˆâ–‹                                                             | 259/10000 [00:51<46:56,  3.46it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.54s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   3%|â–ˆâ–‰                                                             | 317/10000 [01:03<45:58,  3.51it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.95s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|â–ˆâ–ˆâ–                                                            | 378/10000 [01:18<55:48,  2.87it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.77s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|â–ˆâ–ˆâ–                                                          | 390/10000 [01:21<1:00:31,  2.65it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|â–ˆâ–ˆâ–                                                          | 391/10000 [01:22<1:05:39,  2.44it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.70s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|â–ˆâ–ˆâ–‹                                                            | 424/10000 [01:31<30:06,  5.30it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.73s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|â–ˆâ–ˆâ–Š                                                            | 441/10000 [01:36<46:20,  3.44it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.78s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|â–ˆâ–ˆâ–Š                                                            | 454/10000 [01:38<18:33,  8.57it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.68s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|â–ˆâ–ˆâ–‰                                                            | 468/10000 [01:42<46:55,  3.39it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.79s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|â–ˆâ–ˆâ–‰                                                            | 476/10000 [01:44<45:33,  3.48it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.98s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|â–ˆâ–ˆâ–ˆâ–                                                           | 542/10000 [01:59<41:24,  3.81it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.69s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–Œ                                                           | 565/10000 [02:04<37:08,  4.23it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.76s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–‹                                                           | 582/10000 [02:09<50:06,  3.13it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.91s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–‹                                                           | 590/10000 [02:11<34:16,  4.57it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.81s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–‹                                                           | 593/10000 [02:12<28:51,  5.43it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.92s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–Š                                                           | 608/10000 [02:17<48:11,  3.25it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|â–ˆâ–ˆâ–ˆâ–ˆ                                                           | 639/10000 [02:27<49:56,  3.12it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 670/10000 [02:36<37:55,  4.10it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.64s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 727/10000 [02:48<29:54,  5.17it/s]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# ğŸ“¦ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "# =============================================\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from trectools import TrecQrel, TrecRun, TrecEval \n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Memory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ø´\n",
    "# =============================================\n",
    "memory = Memory(location='./cache', verbose=0)\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ir_datasets (MSMARCO)\n",
    "# =============================================\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "queries_path = os.path.expanduser(\"~/.ir_datasets/msmarco-passage/train/queries.tsv\")\n",
    "\n",
    "queries = {}\n",
    "with open(queries_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            queries[parts[0]] = parts[1]\n",
    "\n",
    "qrels = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.relevance > 0:\n",
    "        qrels.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ§¼ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "# =============================================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# =============================================\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª\n",
    "# =============================================\n",
    "tfidf_doc_ids = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/doc_ids_msmarco_train_test.joblib\")\n",
    "tfidf_matrix = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_matrix_msmarco_train_test.joblib\")\n",
    "tfidf_vectorizer = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_vectorizer_msmarco_train_test.joblib\")\n",
    "inverted_index_data = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_inverted_index.joblib\")\n",
    "\n",
    "bert_embeddings = np.load(r\"../data/msmarco_train_test/index/bert/bert_embeddings.npy\")\n",
    "bert_doc_ids = joblib.load(r\"../data/msmarco_train_test/index/bert/doc_ids.joblib\")\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "docs_dict = {}\n",
    "with open(r\"../data/msmarco_train_test/raw/raw_msmarco_train_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            j = json.loads(line)\n",
    "            docs_dict[str(j[\"id\"])] = j[\"text\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# =============================================\n",
    "# âœ³ï¸ ÙÙ„ØªØ±Ø© qrels Ùˆ queries\n",
    "# =============================================\n",
    "available_doc_ids = set(docs_dict.keys())\n",
    "filtered_qrels = {\n",
    "    qid: {docid for docid in docids if docid in available_doc_ids}\n",
    "    for qid, docids in qrels.items()\n",
    "}\n",
    "filtered_qrels = {qid: docids for qid, docids in filtered_qrels.items() if docids}\n",
    "filtered_queries = {qid: queries[qid] for qid in filtered_qrels}\n",
    "\n",
    "qrels = filtered_qrels\n",
    "queries = filtered_queries\n",
    "\n",
    "# Ø£Ø®Ø° Ø£ÙˆÙ„ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø·\n",
    "sample_queries = dict(list(queries.items())[:10000])\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£ØµÙ„ÙŠØ©\n",
    "# =============================================\n",
    "def search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_dict, top_k=10, candidate_size=100):\n",
    "    cleaned_query = advanced_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index_data[\"inverted_index\"]:\n",
    "            postings = inverted_index_data[\"inverted_index\"][term]\n",
    "            for doc_id, score in postings:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score\n",
    "\n",
    "    candidate_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:candidate_size]\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id, _ in candidate_docs if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "\n",
    "    candidate_tfidf_matrix = tfidf_matrix[candidate_indices]\n",
    "    query_vector = tfidf_vectorizer.transform([cleaned_query])\n",
    "    cosine_scores = cosine_similarity(query_vector, candidate_tfidf_matrix).flatten()\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_idx = candidate_indices[idx]\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        doc_text = docs_dict.get(doc_id, \"\")\n",
    "        score = cosine_scores[idx]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "def search_bert(query, top_k=10):\n",
    "    query_embedding = bert_model.encode([query])\n",
    "    bert_scores = cosine_similarity(query_embedding, bert_embeddings).flatten()\n",
    "    top_indices = np.argsort(bert_scores)[::-1][:top_k]\n",
    "    results = [(bert_doc_ids[i], docs_dict.get(bert_doc_ids[i], \"\"), bert_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "def search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    tfidf_scores = cosine_similarity(tfidf_vectorizer.transform([advanced_preprocess(query)]), tfidf_matrix).flatten()\n",
    "    bert_scores = cosine_similarity(bert_model.encode([query]), bert_embeddings).flatten()\n",
    "    combined_scores = tfidf_weight * tfidf_scores + bert_weight * bert_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    results = [(tfidf_doc_ids[i], docs_dict.get(tfidf_doc_ids[i], \"\"), combined_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# =============================================\n",
    "# ğŸ§  ØªØºÙ„ÙŠÙ Ø¨Ø§Ù„ÙƒØ§Ø´\n",
    "# =============================================\n",
    "@memory.cache\n",
    "def cached_search_tfidf(query, top_k=10, candidate_size=100):\n",
    "    return search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, tfidf_doc_ids, docs_dict, top_k, candidate_size)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_bert(query, top_k=10):\n",
    "    return search_bert(query, top_k)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    return search_hybrid(query, tfidf_weight, bert_weight, top_k)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ Ø¯ÙˆØ§Ù„ ÙƒØªØ§Ø¨Ø© run Ùˆ qrel\n",
    "# =============================================\n",
    "def write_qrel_file(qrels, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f\"{qid} 0 {docid} 1\\n\")\n",
    "\n",
    "def write_run_file_threaded(search_fn, queries, run_name, filepath, top_k=10, max_workers=8):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(search_fn, query, top_k=top_k): qid\n",
    "                for qid, query in queries.items()\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Running {run_name}\"):\n",
    "                qid = futures[future]\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    for rank, (doc_id, _, score) in enumerate(results, start=1):\n",
    "                        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error in query {qid}: {e}\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ˆ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "# =============================================\n",
    "qrel_path = \"filtered_msmarco.qrel\"\n",
    "run_tfidf_path = \"run_tfidf.txt\"\n",
    "run_bert_path = \"run_bert.txt\"\n",
    "run_hybrid_path = \"run_hybrid.txt\"\n",
    "\n",
    "write_qrel_file(qrels, qrel_path)\n",
    "write_run_file_threaded(cached_search_tfidf, sample_queries, \"TFIDF\", run_tfidf_path, top_k=10)\n",
    "write_run_file_threaded(cached_search_bert, sample_queries, \"BERT\", run_bert_path, top_k=10)\n",
    "write_run_file_threaded(lambda q, top_k=10: cached_search_hybrid(q, tfidf_weight=0.4, bert_weight=0.6, top_k=top_k), sample_queries, \"Hybrid\", run_hybrid_path, top_k=10)\n",
    "\n",
    "qrel = TrecQrel(qrel_path)\n",
    "runs = {\n",
    "    \"TFIDF\": TrecRun(run_tfidf_path),\n",
    "    \"BERT\": TrecRun(run_bert_path),\n",
    "    \"Hybrid\": TrecRun(run_hybrid_path),\n",
    "}\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, run in runs.items():\n",
    "    evaluation = TrecEval(run, qrel)\n",
    "    model_results = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAP\": evaluation.get_map(),\n",
    "        \"MRR\": evaluation.get_reciprocal_rank(),\n",
    "        \"P@10\": evaluation.get_precision(10),\n",
    "        \"Recall\": evaluation.get_recall(1000)\n",
    "    }\n",
    "    results_table.append(model_results)\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation Results:\")\n",
    "print(tabulate(results_table, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".4f\"))\n",
    "\n",
    "for path in [qrel_path, run_tfidf_path, run_bert_path, run_hybrid_path]:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except OSError as e:\n",
    "        print(f\"âš ï¸ ÙØ´Ù„ Ø­Ø°Ù {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51630-c3c1-4c2f-a037-8a7fffff45a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
