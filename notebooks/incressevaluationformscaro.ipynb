{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5208eab9-7534-4e5b-88f2-76950a4aa1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحميل 10000 وثيقة...\n",
      "✅ تم تحميل 20000 وثيقة...\n",
      "✅ تم تحميل 30000 وثيقة...\n",
      "✅ تم تحميل 40000 وثيقة...\n",
      "✅ تم تحميل 50000 وثيقة...\n",
      "✅ تم تحميل 60000 وثيقة...\n",
      "✅ تم تحميل 70000 وثيقة...\n",
      "✅ تم تحميل 80000 وثيقة...\n",
      "✅ تم تحميل 90000 وثيقة...\n",
      "✅ تم تحميل 100000 وثيقة...\n",
      "✅ تم تحميل 110000 وثيقة...\n",
      "✅ تم تحميل 120000 وثيقة...\n",
      "✅ تم تحميل 130000 وثيقة...\n",
      "✅ تم تحميل 140000 وثيقة...\n",
      "✅ تم تحميل 150000 وثيقة...\n",
      "✅ تم تحميل 160000 وثيقة...\n",
      "✅ تم تحميل 170000 وثيقة...\n",
      "✅ تم تحميل 180000 وثيقة...\n",
      "✅ تم تحميل 190000 وثيقة...\n",
      "✅ تم تحميل 200000 وثيقة...\n",
      "✅ تم تحميل 210000 وثيقة...\n",
      "✅ تم تحميل 220000 وثيقة...\n",
      "✅ تم تحميل 230000 وثيقة...\n",
      "✅ تم تحميل 240000 وثيقة...\n",
      "✅ تم تحميل 250000 وثيقة...\n",
      "✅ تم تحميل 260000 وثيقة...\n",
      "✅ تم تحميل 270000 وثيقة...\n",
      "✅ تم تحميل 280000 وثيقة...\n",
      "✅ تم تحميل 290000 وثيقة...\n",
      "✅ تم تحميل 300000 وثيقة...\n",
      "✅ تم تحميل 310000 وثيقة...\n",
      "✅ تم تحميل 320000 وثيقة...\n",
      "✅ تم تحميل 330000 وثيقة...\n",
      "✅ تم تحميل 340000 وثيقة...\n",
      "✅ تم تحميل 350000 وثيقة...\n",
      "✅ تم تحميل 360000 وثيقة...\n",
      "✅ تم تحميل 370000 وثيقة...\n",
      "✅ تم تحميل 380000 وثيقة...\n",
      "✅ تم تحميل 390000 وثيقة...\n",
      "✅ تم تحميل 400000 وثيقة...\n",
      "✅ تم تحميل 410000 وثيقة...\n",
      "✅ تم تحميل 420000 وثيقة...\n",
      "✅ تم تحميل 430000 وثيقة...\n",
      "✅ تم تحميل 440000 وثيقة...\n",
      "✅ تم تحميل 450000 وثيقة...\n",
      "✅ تم تحميل 460000 وثيقة...\n",
      "✅ تم تحميل 470000 وثيقة...\n",
      "✅ تم تحميل 480000 وثيقة...\n",
      "✅ تم تحميل 490000 وثيقة...\n",
      "✅ تم تحميل 500000 وثيقة...\n",
      "\n",
      "🧹 بدء تنظيف النصوص وتحويلها إلى Tokens...\n",
      "\n",
      "📁 تم حفظ البيانات بنجاح في الملف:\n",
      "..\\data\\msmarco_train_test\\processed\\tokenized_msmarco.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# تحميل موارد NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. تحميل مجموعة البيانات\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "# 2. قراءة الوثائق وتحويلها إلى قائمة\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 500_000\n",
    "seen_ids = set()  # لمنع التكرار\n",
    "\n",
    "# 3. تنظيف Unicode المشوه\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        text = html.unescape(text)  # إزالة HTML entities مثل &amp;\n",
    "        text = unicodedata.normalize(\"NFKD\", text)  # Normalize Unicode\n",
    "        text = ''.join(c for c in text if c.isprintable())  # إزالة الأحرف غير القابلة للطباعة\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. أدوات المعالجة اللغوية\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# 5. دالة التنظيف المتقدمة – تُرجع قائمة كلمات (tokens)\n",
    "def advanced_preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # إزالة الروابط\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # إزالة الإيميلات\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # إزالة علامات HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # إزالة الأرقام\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # إزالة علامات الترقيم والرموز الخاصة\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # إزالة التكرار الزائد في الحروف\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # إزالة الفراغات الزائدة\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenization + Stopword Removal + Stemming\n",
    "    tokens = text.split()\n",
    "    processed = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return processed\n",
    "\n",
    "# 6. تحميل وتنظيف الوثائق\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        clean_original = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": clean_original\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"✅ تم تحميل {count} وثيقة...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# 7. تحويل إلى DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 8. تطبيق التنظيف المتقدم => tokens\n",
    "print(\"\\n🧹 بدء تنظيف النصوص وتحويلها إلى Tokens...\")\n",
    "df['tokens'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 9. إزالة التكرارات حسب ID\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 10. حفظ النتائج على شكل JSON\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\processed\"\n",
    "output_file = os.path.join(output_dir, \"tokenized_msmarco.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'tokens']].to_json(output_file, orient='records', lines=False, force_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(f\"\\n📁 تم حفظ البيانات بنجاح في الملف:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51043531-28ab-4d95-89b0-b8abf8b11f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ ملفات TF-IDF بنجاح.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import advanced_preprocess\n",
    "\n",
    "# 1. تحميل البيانات\n",
    "input_file = r\"..\\data\\msmarco_train_test\\processed\\tokenized_msmarco.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. استخراج النصوص والمعرفات\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [\" \".join(doc['tokens']) for doc in data]  # نصوص نصية\n",
    "\n",
    "# 3. إعداد TfidfVectorizer مع preprocess كـ tokenizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=advanced_preprocess,   # دالة تقسيم الكلمات مع التنظيف\n",
    "    preprocessor=None,      # لا تستخدم preprocessor آخر\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    lowercase=False         # لأن preprocess تقوم بتحويل الحروف إلى صغيرة بالفعل\n",
    ")\n",
    "\n",
    "# 4. تطبيق TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. حفظ النتائج\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_msmarco_train_test.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_msmarco_train_test.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_msmarco_train_test.joblib\"))\n",
    "\n",
    "print(\"✅ تم حفظ ملفات TF-IDF بنجاح.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02380e3d-bd82-4470-b1e2-39a17551f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. تحديد مسارات الملفات ---\n",
      "مجلد الإخراج موجود: ..\\data\\msmarco_train_test\\index\\TFIDF\n",
      "\n",
      "--- 2. جاري تحميل الملفات ---\n",
      "تم تحميل الملفات بنجاح.\n",
      "عدد المستندات: 500000\n",
      "أبعاد مصفوفة TF-IDF: (500000, 91371)\n",
      "\n",
      "--- 3. جاري بناء الفهرس المعكوس ---\n",
      "بدء عملية التكرار على مصفوفة TF-IDF (قد تستغرق بعض الوقت)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f503df2c23473b89019da9297d0d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "بناء الفهرس المعكوس:   0%|          | 0/11624617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جاري فرز الوثائق لكل مصطلح حسب درجة TF-IDF...\n",
      "تم بناء الفهرس المعكوس بنجاح.\n",
      "عدد الكلمات الفريدة في الفهرس المعكوس: 91371\n",
      "\n",
      "--- 4. إعداد البيانات للحفظ ---\n",
      "\n",
      "--- 5. جاري حفظ الفهرس المعكوس ---\n",
      "✅ تم حفظ الفهرس المعكوس بنجاح في: ..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_inverted_index.joblib\n",
      "\n",
      "--- 6. التحقق من الفهرس المحفوظ ---\n",
      "✅ تم تحميل الفهرس للتحقق.\n",
      "عدد الكلمات في الفهرس: 91371\n",
      "مثال على إدخال في الفهرس المعكوس ('presenc'): [('409792', 0.4636566116093226), ('580660', 0.44391864539155124), ('774505', 0.43167140720627645), ('569503', 0.41676428533006127), ('585454', 0.3902043674130837)]\n",
      "\n",
      "--- ✅ انتهى تنفيذ الكود بنجاح ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------ التحميلات الأساسية ------------------\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import advanced_preprocess\n",
    "\n",
    "# ------------------ 1. تحديد المسارات ------------------\n",
    "print(\"--- 1. تحديد مسارات الملفات ---\")\n",
    "doc_ids_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\doc_ids_msmarco_train_test.joblib\"\n",
    "tfidf_matrix_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_matrix_msmarco_train_test.joblib\"\n",
    "tfidf_vectorizer_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_vectorizer_msmarco_train_test.joblib\"\n",
    "\n",
    "output_inverted_index_path = r\"..\\data\\msmarco_train_test\\index\\TFIDF\\tfidf_inverted_index.joblib\"\n",
    "\n",
    "output_dir = os.path.dirname(output_inverted_index_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"تم إنشاء مجلد الإخراج: {output_dir}\")\n",
    "else:\n",
    "    print(f\"مجلد الإخراج موجود: {output_dir}\")\n",
    "\n",
    "# ------------------ 2. تحميل الملفات ------------------\n",
    "print(\"\\n--- 2. جاري تحميل الملفات ---\")\n",
    "try:\n",
    "    doc_ids = joblib.load(doc_ids_path)\n",
    "    tfidf_matrix = joblib.load(tfidf_matrix_path)\n",
    "    tfidf_vectorizer = joblib.load(tfidf_vectorizer_path)\n",
    "    print(\"تم تحميل الملفات بنجاح.\")\n",
    "    print(f\"عدد المستندات: {len(doc_ids)}\")\n",
    "    print(f\"أبعاد مصفوفة TF-IDF: {tfidf_matrix.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"خطأ: لم يتم العثور على أحد الملفات. يرجى التحقق من المسارات.\")\n",
    "    print(f\"المسار المفقود: {e.filename}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"حدث خطأ أثناء تحميل الملفات: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ------------------ 3. بناء الفهرس المعكوس ------------------\n",
    "print(\"\\n--- 3. جاري بناء الفهرس المعكوس ---\")\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "inverted_index = {}\n",
    "\n",
    "tfidf_matrix_coo = tfidf_matrix.tocoo()\n",
    "print(\"بدء عملية التكرار على مصفوفة TF-IDF (قد تستغرق بعض الوقت)...\")\n",
    "\n",
    "for doc_idx, term_idx, tfidf_score in tqdm(zip(tfidf_matrix_coo.row, tfidf_matrix_coo.col, tfidf_matrix_coo.data),\n",
    "                                           total=len(tfidf_matrix_coo.data),\n",
    "                                           desc=\"بناء الفهرس المعكوس\"):\n",
    "    term = feature_names[term_idx]\n",
    "    doc_id = doc_ids[doc_idx]\n",
    "    if term not in inverted_index:\n",
    "        inverted_index[term] = []\n",
    "    inverted_index[term].append((doc_id, float(tfidf_score)))\n",
    "\n",
    "print(\"جاري فرز الوثائق لكل مصطلح حسب درجة TF-IDF...\")\n",
    "for term in inverted_index:\n",
    "    inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"تم بناء الفهرس المعكوس بنجاح.\")\n",
    "print(f\"عدد الكلمات الفريدة في الفهرس المعكوس: {len(inverted_index)}\")\n",
    "\n",
    "# ------------------ 4. إعداد البيانات للحفظ ------------------\n",
    "print(\"\\n--- 4. إعداد البيانات للحفظ ---\")\n",
    "inverted_index_data = {\n",
    "    \"inverted_index\": inverted_index,\n",
    "    \"num_documents\": len(doc_ids),\n",
    "    \"num_terms\": len(inverted_index),\n",
    "    \"vocabulary_size\": len(feature_names),\n",
    "    \"vectorizer_vocabulary\": dict(tfidf_vectorizer.vocabulary_)\n",
    "}\n",
    "\n",
    "# ------------------ 5. حفظ الفهرس ------------------\n",
    "print(\"\\n--- 5. جاري حفظ الفهرس المعكوس ---\")\n",
    "try:\n",
    "    joblib.dump(inverted_index_data, output_inverted_index_path)\n",
    "    print(f\"✅ تم حفظ الفهرس المعكوس بنجاح في: {output_inverted_index_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ حدث خطأ أثناء حفظ الفهرس المعكوس: {e}\")\n",
    "\n",
    "# ------------------ 6. التحقق من الحفظ ------------------\n",
    "print(\"\\n--- 6. التحقق من الفهرس المحفوظ ---\")\n",
    "try:\n",
    "    loaded_data = joblib.load(output_inverted_index_path)\n",
    "    loaded_index = loaded_data[\"inverted_index\"]\n",
    "    print(\"✅ تم تحميل الفهرس للتحقق.\")\n",
    "    print(f\"عدد الكلمات في الفهرس: {len(loaded_index)}\")\n",
    "\n",
    "    example_term = next(iter(loaded_index))\n",
    "    print(f\"مثال على إدخال في الفهرس المعكوس ('{example_term}'): {loaded_index[example_term][:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ حدث خطأ أثناء التحقق: {e}\")\n",
    "\n",
    "print(\"\\n--- ✅ انتهى تنفيذ الكود بنجاح ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13985db-aed3-4f1d-9e81-8b3069632404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحميل 10000 وثيقة...\n",
      "✅ تم تحميل 20000 وثيقة...\n",
      "✅ تم تحميل 30000 وثيقة...\n",
      "✅ تم تحميل 40000 وثيقة...\n",
      "✅ تم تحميل 50000 وثيقة...\n",
      "✅ تم تحميل 60000 وثيقة...\n",
      "✅ تم تحميل 70000 وثيقة...\n",
      "✅ تم تحميل 80000 وثيقة...\n",
      "✅ تم تحميل 90000 وثيقة...\n",
      "✅ تم تحميل 100000 وثيقة...\n",
      "✅ تم تحميل 110000 وثيقة...\n",
      "✅ تم تحميل 120000 وثيقة...\n",
      "✅ تم تحميل 130000 وثيقة...\n",
      "✅ تم تحميل 140000 وثيقة...\n",
      "✅ تم تحميل 150000 وثيقة...\n",
      "✅ تم تحميل 160000 وثيقة...\n",
      "✅ تم تحميل 170000 وثيقة...\n",
      "✅ تم تحميل 180000 وثيقة...\n",
      "✅ تم تحميل 190000 وثيقة...\n",
      "✅ تم تحميل 200000 وثيقة...\n",
      "✅ تم تحميل 210000 وثيقة...\n",
      "✅ تم تحميل 220000 وثيقة...\n",
      "✅ تم تحميل 230000 وثيقة...\n",
      "✅ تم تحميل 240000 وثيقة...\n",
      "✅ تم تحميل 250000 وثيقة...\n",
      "✅ تم تحميل 260000 وثيقة...\n",
      "✅ تم تحميل 270000 وثيقة...\n",
      "✅ تم تحميل 280000 وثيقة...\n",
      "✅ تم تحميل 290000 وثيقة...\n",
      "✅ تم تحميل 300000 وثيقة...\n",
      "✅ تم تحميل 310000 وثيقة...\n",
      "✅ تم تحميل 320000 وثيقة...\n",
      "✅ تم تحميل 330000 وثيقة...\n",
      "✅ تم تحميل 340000 وثيقة...\n",
      "✅ تم تحميل 350000 وثيقة...\n",
      "✅ تم تحميل 360000 وثيقة...\n",
      "✅ تم تحميل 370000 وثيقة...\n",
      "✅ تم تحميل 380000 وثيقة...\n",
      "✅ تم تحميل 390000 وثيقة...\n",
      "✅ تم تحميل 400000 وثيقة...\n",
      "✅ تم تحميل 410000 وثيقة...\n",
      "✅ تم تحميل 420000 وثيقة...\n",
      "✅ تم تحميل 430000 وثيقة...\n",
      "✅ تم تحميل 440000 وثيقة...\n",
      "✅ تم تحميل 450000 وثيقة...\n",
      "✅ تم تحميل 460000 وثيقة...\n",
      "✅ تم تحميل 470000 وثيقة...\n",
      "✅ تم تحميل 480000 وثيقة...\n",
      "✅ تم تحميل 490000 وثيقة...\n",
      "✅ تم تحميل 500000 وثيقة...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "seen_ids = set()\n",
    "\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "count = 0\n",
    "\n",
    "docs_data = []\n",
    "\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "while count < 500000:\n",
    "\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # تجاهل التكرارات\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"✅ تم تحميل {count} وثيقة...\")\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# 6. إنشاء DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "df.drop(df.loc[df['id']==''].index,inplace=True)\n",
    "\n",
    "df['id']=df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adafee3-aabe-4728-8f4f-b6f99776bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"..\\data\\msmarco_train_test\\raw\"\n",
    "output_file = os.path.join(output_dir, \"raw_msmarco_train_test.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_json(output_file, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f496196-e4c3-4596-8a4f-07461622bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TFIDF:   0%|                                                                 | 5/10000 [00:00<04:23, 37.90it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.65s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 16/10000 [00:02<25:05,  6.63it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.72s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 17/10000 [00:02<32:09,  5.17it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 18/10000 [00:03<46:47,  3.56it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.66s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|                                                                | 19/10000 [00:03<54:49,  3.03it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   0%|▎                                                               | 49/10000 [00:09<44:57,  3.69it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.62s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▍                                                               | 66/10000 [00:11<20:42,  8.00it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.53s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▍                                                               | 69/10000 [00:12<33:46,  4.90it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▍                                                               | 73/10000 [00:13<26:13,  6.31it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▌                                                               | 84/10000 [00:15<20:04,  8.23it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▌                                                               | 86/10000 [00:15<32:51,  5.03it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.61s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▌                                                               | 93/10000 [00:17<42:55,  3.85it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▉                                                              | 146/10000 [00:27<36:25,  4.51it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.87s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   1%|▉                                                              | 149/10000 [00:27<36:35,  4.49it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|█                                                              | 178/10000 [00:34<41:13,  3.97it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.52s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|█▏                                                             | 193/10000 [00:36<19:31,  8.37it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.60s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   2%|█▎                                                             | 199/10000 [00:37<41:36,  3.93it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   3%|█▋                                                             | 259/10000 [00:51<46:56,  3.46it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.54s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   3%|█▉                                                             | 317/10000 [01:03<45:58,  3.51it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.95s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|██▍                                                            | 378/10000 [01:18<55:48,  2.87it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.77s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|██▍                                                          | 390/10000 [01:21<1:00:31,  2.65it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|██▍                                                          | 391/10000 [01:22<1:05:39,  2.44it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.70s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|██▋                                                            | 424/10000 [01:31<30:06,  5.30it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.73s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   4%|██▊                                                            | 441/10000 [01:36<46:20,  3.44it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.78s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|██▊                                                            | 454/10000 [01:38<18:33,  8.57it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.68s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|██▉                                                            | 468/10000 [01:42<46:55,  3.39it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.79s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|██▉                                                            | 476/10000 [01:44<45:33,  3.48it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.98s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   5%|███▍                                                           | 542/10000 [01:59<41:24,  3.81it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.69s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|███▌                                                           | 565/10000 [02:04<37:08,  4.23it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.76s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|███▋                                                           | 582/10000 [02:09<50:06,  3.13it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.91s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|███▋                                                           | 590/10000 [02:11<34:16,  4.57it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.81s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|███▋                                                           | 593/10000 [02:12<28:51,  5.43it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.92s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|███▊                                                           | 608/10000 [02:17<48:11,  3.25it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   6%|████                                                           | 639/10000 [02:27<49:56,  3.12it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   7%|████▏                                                          | 670/10000 [02:36<37:55,  4.10it/s]C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:577: UserWarning: Persisting input arguments took 0.64s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.\n",
      "  return self._cached_call(args, kwargs, shelving=False)[0]\n",
      "Running TFIDF:   7%|████▌                                                          | 727/10000 [02:48<29:54,  5.17it/s]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 📦 المكتبات المطلوبة\n",
    "# =============================================\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from trectools import TrecQrel, TrecRun, TrecEval \n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Memory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# =============================================\n",
    "# ⚙️ تهيئة الكاش\n",
    "# =============================================\n",
    "memory = Memory(location='./cache', verbose=0)\n",
    "\n",
    "# =============================================\n",
    "# ⚙️ تحميل بيانات ir_datasets (MSMARCO)\n",
    "# =============================================\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "queries_path = os.path.expanduser(\"~/.ir_datasets/msmarco-passage/train/queries.tsv\")\n",
    "\n",
    "queries = {}\n",
    "with open(queries_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            queries[parts[0]] = parts[1]\n",
    "\n",
    "qrels = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.relevance > 0:\n",
    "        qrels.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
    "\n",
    "# =============================================\n",
    "# 🧼 دالة التنظيف\n",
    "# =============================================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# =============================================\n",
    "# تحميل ملفات التمثيلات\n",
    "# =============================================\n",
    "tfidf_doc_ids = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/doc_ids_msmarco_train_test.joblib\")\n",
    "tfidf_matrix = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_matrix_msmarco_train_test.joblib\")\n",
    "tfidf_vectorizer = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_vectorizer_msmarco_train_test.joblib\")\n",
    "inverted_index_data = joblib.load(r\"../data/msmarco_train_test/index/TFIDF/tfidf_inverted_index.joblib\")\n",
    "\n",
    "bert_embeddings = np.load(r\"../data/msmarco_train_test/index/bert/bert_embeddings.npy\")\n",
    "bert_doc_ids = joblib.load(r\"../data/msmarco_train_test/index/bert/doc_ids.joblib\")\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "docs_dict = {}\n",
    "with open(r\"../data/msmarco_train_test/raw/raw_msmarco_train_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            j = json.loads(line)\n",
    "            docs_dict[str(j[\"id\"])] = j[\"text\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# =============================================\n",
    "# ✳️ فلترة qrels و queries\n",
    "# =============================================\n",
    "available_doc_ids = set(docs_dict.keys())\n",
    "filtered_qrels = {\n",
    "    qid: {docid for docid in docids if docid in available_doc_ids}\n",
    "    for qid, docids in qrels.items()\n",
    "}\n",
    "filtered_qrels = {qid: docids for qid, docids in filtered_qrels.items() if docids}\n",
    "filtered_queries = {qid: queries[qid] for qid in filtered_qrels}\n",
    "\n",
    "qrels = filtered_qrels\n",
    "queries = filtered_queries\n",
    "\n",
    "# أخذ أول 5000 استعلام فقط\n",
    "sample_queries = dict(list(queries.items())[:10000])\n",
    "\n",
    "# =============================================\n",
    "# 🔍 دوال البحث الأصلية\n",
    "# =============================================\n",
    "def search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_dict, top_k=10, candidate_size=100):\n",
    "    cleaned_query = advanced_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index_data[\"inverted_index\"]:\n",
    "            postings = inverted_index_data[\"inverted_index\"][term]\n",
    "            for doc_id, score in postings:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score\n",
    "\n",
    "    candidate_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:candidate_size]\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id, _ in candidate_docs if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "\n",
    "    candidate_tfidf_matrix = tfidf_matrix[candidate_indices]\n",
    "    query_vector = tfidf_vectorizer.transform([cleaned_query])\n",
    "    cosine_scores = cosine_similarity(query_vector, candidate_tfidf_matrix).flatten()\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_idx = candidate_indices[idx]\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        doc_text = docs_dict.get(doc_id, \"\")\n",
    "        score = cosine_scores[idx]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "def search_bert(query, top_k=10):\n",
    "    query_embedding = bert_model.encode([query])\n",
    "    bert_scores = cosine_similarity(query_embedding, bert_embeddings).flatten()\n",
    "    top_indices = np.argsort(bert_scores)[::-1][:top_k]\n",
    "    results = [(bert_doc_ids[i], docs_dict.get(bert_doc_ids[i], \"\"), bert_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "def search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    tfidf_scores = cosine_similarity(tfidf_vectorizer.transform([advanced_preprocess(query)]), tfidf_matrix).flatten()\n",
    "    bert_scores = cosine_similarity(bert_model.encode([query]), bert_embeddings).flatten()\n",
    "    combined_scores = tfidf_weight * tfidf_scores + bert_weight * bert_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    results = [(tfidf_doc_ids[i], docs_dict.get(tfidf_doc_ids[i], \"\"), combined_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# =============================================\n",
    "# 🧠 تغليف بالكاش\n",
    "# =============================================\n",
    "@memory.cache\n",
    "def cached_search_tfidf(query, top_k=10, candidate_size=100):\n",
    "    return search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, tfidf_doc_ids, docs_dict, top_k, candidate_size)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_bert(query, top_k=10):\n",
    "    return search_bert(query, top_k)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    return search_hybrid(query, tfidf_weight, bert_weight, top_k)\n",
    "\n",
    "# =============================================\n",
    "# 📁 دوال كتابة run و qrel\n",
    "# =============================================\n",
    "def write_qrel_file(qrels, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f\"{qid} 0 {docid} 1\\n\")\n",
    "\n",
    "def write_run_file_threaded(search_fn, queries, run_name, filepath, top_k=10, max_workers=8):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(search_fn, query, top_k=top_k): qid\n",
    "                for qid, query in queries.items()\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Running {run_name}\"):\n",
    "                qid = futures[future]\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    for rank, (doc_id, _, score) in enumerate(results, start=1):\n",
    "                        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in query {qid}: {e}\")\n",
    "\n",
    "# =============================================\n",
    "# 📈 التقييم\n",
    "# =============================================\n",
    "qrel_path = \"filtered_msmarco.qrel\"\n",
    "run_tfidf_path = \"run_tfidf.txt\"\n",
    "run_bert_path = \"run_bert.txt\"\n",
    "run_hybrid_path = \"run_hybrid.txt\"\n",
    "\n",
    "write_qrel_file(qrels, qrel_path)\n",
    "write_run_file_threaded(cached_search_tfidf, sample_queries, \"TFIDF\", run_tfidf_path, top_k=10)\n",
    "write_run_file_threaded(cached_search_bert, sample_queries, \"BERT\", run_bert_path, top_k=10)\n",
    "write_run_file_threaded(lambda q, top_k=10: cached_search_hybrid(q, tfidf_weight=0.4, bert_weight=0.6, top_k=top_k), sample_queries, \"Hybrid\", run_hybrid_path, top_k=10)\n",
    "\n",
    "qrel = TrecQrel(qrel_path)\n",
    "runs = {\n",
    "    \"TFIDF\": TrecRun(run_tfidf_path),\n",
    "    \"BERT\": TrecRun(run_bert_path),\n",
    "    \"Hybrid\": TrecRun(run_hybrid_path),\n",
    "}\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, run in runs.items():\n",
    "    evaluation = TrecEval(run, qrel)\n",
    "    model_results = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAP\": evaluation.get_map(),\n",
    "        \"MRR\": evaluation.get_reciprocal_rank(),\n",
    "        \"P@10\": evaluation.get_precision(10),\n",
    "        \"Recall\": evaluation.get_recall(1000)\n",
    "    }\n",
    "    results_table.append(model_results)\n",
    "\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "print(tabulate(results_table, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".4f\"))\n",
    "\n",
    "for path in [qrel_path, run_tfidf_path, run_bert_path, run_hybrid_path]:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except OSError as e:\n",
    "        print(f\"⚠️ فشل حذف {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51630-c3c1-4c2f-a037-8a7fffff45a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
