{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d37e3f-a8c1-4612-add5-b98dde8e3f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحميل 10000 وثيقة...\n",
      "✅ تم تحميل 20000 وثيقة...\n",
      "✅ تم تحميل 30000 وثيقة...\n",
      "✅ تم تحميل 40000 وثيقة...\n",
      "✅ تم تحميل 50000 وثيقة...\n",
      "✅ تم تحميل 60000 وثيقة...\n",
      "✅ تم تحميل 70000 وثيقة...\n",
      "✅ تم تحميل 80000 وثيقة...\n",
      "✅ تم تحميل 90000 وثيقة...\n",
      "✅ تم تحميل 100000 وثيقة...\n",
      "✅ تم تحميل 110000 وثيقة...\n",
      "✅ تم تحميل 120000 وثيقة...\n",
      "✅ تم تحميل 130000 وثيقة...\n",
      "✅ تم تحميل 140000 وثيقة...\n",
      "✅ تم تحميل 150000 وثيقة...\n",
      "✅ تم تحميل 160000 وثيقة...\n",
      "✅ تم تحميل 170000 وثيقة...\n",
      "✅ تم تحميل 180000 وثيقة...\n",
      "✅ تم تحميل 190000 وثيقة...\n",
      "✅ تم تحميل 200000 وثيقة...\n",
      "✅ تم تحميل 210000 وثيقة...\n",
      "✅ تم تحميل 220000 وثيقة...\n",
      "✅ تم تحميل 230000 وثيقة...\n",
      "✅ تم تحميل 240000 وثيقة...\n",
      "✅ تم تحميل 250000 وثيقة...\n",
      "✅ تم تحميل 260000 وثيقة...\n",
      "✅ تم تحميل 270000 وثيقة...\n",
      "✅ تم تحميل 280000 وثيقة...\n",
      "✅ تم تحميل 290000 وثيقة...\n",
      "✅ تم تحميل 300000 وثيقة...\n",
      "✅ تم تحميل 310000 وثيقة...\n",
      "✅ تم تحميل 320000 وثيقة...\n",
      "✅ تم تحميل 330000 وثيقة...\n",
      "✅ تم تحميل 340000 وثيقة...\n",
      "✅ تم تحميل 350000 وثيقة...\n",
      "✅ تم تحميل 360000 وثيقة...\n",
      "✅ تم تحميل 370000 وثيقة...\n",
      "✅ تم تحميل 380000 وثيقة...\n",
      "✅ تم تحميل 390000 وثيقة...\n",
      "✅ تم تحميل 400000 وثيقة...\n",
      "✅ تم تحميل 410000 وثيقة...\n",
      "✅ تم تحميل 420000 وثيقة...\n",
      "✅ تم تحميل 430000 وثيقة...\n",
      "✅ تم تحميل 440000 وثيقة...\n",
      "✅ تم تحميل 450000 وثيقة...\n",
      "✅ تم تحميل 460000 وثيقة...\n",
      "✅ تم تحميل 470000 وثيقة...\n",
      "✅ تم تحميل 480000 وثيقة...\n",
      "✅ تم تحميل 490000 وثيقة...\n",
      "✅ تم تحميل 500000 وثيقة...\n",
      "\n",
      "🧹 بدء تنظيف النصوص...\n",
      "\n",
      "📁 تم حفظ البيانات بنجاح في الملف:\n",
      "..\\data\\msmarco_train\\processed\\processed_msmarco_train.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# تحميل موارد NLTK إذا لم تكن محمّلة\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. تحميل مجموعة البيانات\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "# 2. قراءة 200 ألف وثيقة وتحويلها إلى قائمة من القواميس\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 500_000\n",
    "seen_ids = set()  # لتتبع المعرّفات التي تمت رؤيتها\n",
    "\n",
    "# 3. إصلاح النصوص المشوهة\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. تنظيف النصوص بشكل احترافي\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    # فك ترميز HTML مثل &amp; و &quot;\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # إزالة الحروف الغير قابلة للطباعة\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "\n",
    "    # تحويل النص إلى حروف صغيرة\n",
    "    text = text.lower()\n",
    "\n",
    "    # إزالة الروابط\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # إزالة الإيميلات\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # إزالة علامات HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # إزالة الرموز والرقم المفرد\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # إزالة التكرارات الزائدة (مثل cooool → cool)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # إزالة الفراغات الزائدة\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # إزالة الكلمات الشائعة والكلمات القصيرة\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 5. تحميل الوثائق وتنظيفها\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # تجاهل التكرارات\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"✅ تم تحميل {count} وثيقة...\")\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# 6. إنشاء DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 7. تنظيف النصوص المتقدمة\n",
    "print(\"\\n🧹 بدء تنظيف النصوص...\")\n",
    "df['clean_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 8. إزالة أي تكرارات محتملة (كإجراء إضافي)\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 9. حفظ البيانات\n",
    "output_dir = r\"..\\data\\msmarco_train\\processed\"\n",
    "output_file = os.path.join(output_dir, \"processed_msmarco_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'clean_text']].to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\n📁 تم حفظ البيانات بنجاح في الملف:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce83516-0d19-43b8-966c-91d4599037b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ ملفات TF-IDF بنجاح.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. تحميل البيانات\n",
    "input_file = r\"C:\\Users\\Mohammad Mihdi\\Desktop\\Projects\\IR\\data\\msmarco_train\\processed\\processed_msmarco_train.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 2. استخراج النصوص والمعرفات\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [doc['clean_text'] for doc in data]\n",
    "\n",
    "# 3. استخدام TF-IDF مع دالة التنظيف\n",
    "\n",
    "import html\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=advanced_preprocess,\n",
    "    tokenizer=str.split,\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# 4. تطبيق TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. حفظ النتائج\n",
    "output_dir = r\"C:\\Users\\Mohammad Mihdi\\Desktop\\Projects\\IR\\data\\msmarco_train\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_msmarco_train.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_msmarco_train.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_msmarco_train.joblib\"))\n",
    "\n",
    "print(\"✅ تم حفظ ملفات TF-IDF بنجاح.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e986394e-ed85-4861-84cd-08bdbbbf3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohammad\n",
      "[nltk_data]     Mihdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحميل 10000 وثيقة...\n",
      "✅ تم تحميل 20000 وثيقة...\n",
      "✅ تم تحميل 30000 وثيقة...\n",
      "✅ تم تحميل 40000 وثيقة...\n",
      "✅ تم تحميل 50000 وثيقة...\n",
      "✅ تم تحميل 60000 وثيقة...\n",
      "✅ تم تحميل 70000 وثيقة...\n",
      "✅ تم تحميل 80000 وثيقة...\n",
      "✅ تم تحميل 90000 وثيقة...\n",
      "✅ تم تحميل 100000 وثيقة...\n",
      "✅ تم تحميل 110000 وثيقة...\n",
      "✅ تم تحميل 120000 وثيقة...\n",
      "✅ تم تحميل 130000 وثيقة...\n",
      "✅ تم تحميل 140000 وثيقة...\n",
      "✅ تم تحميل 150000 وثيقة...\n",
      "✅ تم تحميل 160000 وثيقة...\n",
      "✅ تم تحميل 170000 وثيقة...\n",
      "✅ تم تحميل 180000 وثيقة...\n",
      "✅ تم تحميل 190000 وثيقة...\n",
      "✅ تم تحميل 200000 وثيقة...\n",
      "✅ تم تحميل 210000 وثيقة...\n",
      "✅ تم تحميل 220000 وثيقة...\n",
      "✅ تم تحميل 230000 وثيقة...\n",
      "✅ تم تحميل 240000 وثيقة...\n",
      "✅ تم تحميل 250000 وثيقة...\n",
      "✅ تم تحميل 260000 وثيقة...\n",
      "✅ تم تحميل 270000 وثيقة...\n",
      "✅ تم تحميل 280000 وثيقة...\n",
      "✅ تم تحميل 290000 وثيقة...\n",
      "✅ تم تحميل 300000 وثيقة...\n",
      "✅ تم تحميل 310000 وثيقة...\n",
      "✅ تم تحميل 320000 وثيقة...\n",
      "✅ تم تحميل 330000 وثيقة...\n",
      "✅ تم تحميل 340000 وثيقة...\n",
      "✅ تم تحميل 350000 وثيقة...\n",
      "✅ تم تحميل 360000 وثيقة...\n",
      "✅ تم تحميل 370000 وثيقة...\n",
      "📌 انتهت الوثائق الموجودة في الداتا سيت.\n",
      "\n",
      "🧹 بدء تنظيف النصوص...\n",
      "\n",
      "📁 تم حفظ البيانات بنجاح في الملف:\n",
      "..\\data\\antique_train\\processed\\processed_antique_train.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# تحميل موارد NLTK إذا لم تكن محمّلة\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. تحميل مجموعة البيانات\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "# 2. قراءة 200 ألف وثيقة وتحويلها إلى قائمة من القواميس\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 405_000\n",
    "seen_ids = set()  # لتتبع المعرّفات التي تمت رؤيتها\n",
    "\n",
    "# 3. إصلاح النصوص المشوهة\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. تنظيف النصوص بشكل احترافي\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    # فك ترميز HTML مثل &amp; و &quot;\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # إزالة الحروف الغير قابلة للطباعة\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "\n",
    "    # تحويل النص إلى حروف صغيرة\n",
    "    text = text.lower()\n",
    "\n",
    "    # إزالة الروابط\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # إزالة الإيميلات\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # إزالة علامات HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # إزالة الرموز والرقم المفرد\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # إزالة التكرارات الزائدة (مثل cooool → cool)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # إزالة الفراغات الزائدة\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # إزالة الكلمات الشائعة والكلمات القصيرة\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 5. تحميل الوثائق وتنظيفها\n",
    "# 5. تحميل الوثائق وتنظيفها\n",
    "doc_iterator = dataset.docs_iter()\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "    except StopIteration:\n",
    "        print(\"📌 انتهت الوثائق الموجودة في الداتا سيت.\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue  # تجاوز أي خطأ غير متوقع\n",
    "\n",
    "    if doc.doc_id in seen_ids:\n",
    "        continue\n",
    "\n",
    "    seen_ids.add(doc.doc_id)\n",
    "    text_clean = safe_text(doc.text)\n",
    "    docs_data.append({\n",
    "        \"id\": doc.doc_id,\n",
    "        \"text\": text_clean\n",
    "    })\n",
    "    count += 1\n",
    "\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"✅ تم تحميل {count} وثيقة...\")\n",
    "\n",
    "\n",
    "# 6. إنشاء DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 7. تنظيف النصوص المتقدمة\n",
    "print(\"\\n🧹 بدء تنظيف النصوص...\")\n",
    "df['clean_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 8. إزالة أي تكرارات محتملة (كإجراء إضافي)\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 9. حفظ البيانات\n",
    "output_dir = r\"..\\data\\antique_train\\processed\"\n",
    "output_file = os.path.join(output_dir, \"processed_antique_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'clean_text']].to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\n📁 تم حفظ البيانات بنجاح في الملف:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4feb8fb-a6ad-4723-a4de-bc0cbf0a00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ ملفات TF-IDF بنجاح.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. تحميل البيانات\n",
    "input_file = r\"..\\data\\antique_train\\processed\\processed_antique_train.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 2. استخراج النصوص والمعرفات\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [doc['clean_text'] for doc in data]\n",
    "\n",
    "# 3. استخدام TF-IDF مع دالة التنظيف\n",
    "\n",
    "import html\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=advanced_preprocess,\n",
    "    tokenizer=str.split,\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# 4. تطبيق TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. حفظ النتائج\n",
    "output_dir = r\"..\\data\\antique_train\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_antique_train.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_antique_train.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_antique_train.joblib\"))\n",
    "\n",
    "print(\"✅ تم حفظ ملفات TF-IDF بنجاح.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a16278-5306-4aba-9587-9bf2dfb68974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ ملفات TF-IDF بنجاح.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import preprocess\n",
    "\n",
    "# 1. تحميل البيانات\n",
    "input_file = r\"..\\data\\beir_quora_test\\processed\\processed_beir_quora_test.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. استخراج النصوص والمعرفات\n",
    "doc_ids = [doc['doc_id'] for doc in data]\n",
    "texts = [\" \".join(doc['tokens']) for doc in data]  # نصوص نصية\n",
    "\n",
    "# 3. إعداد TfidfVectorizer مع preprocess كـ tokenizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocess,   # دالة تقسيم الكلمات مع التنظيف\n",
    "    preprocessor=None,      # لا تستخدم preprocessor آخر\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    lowercase=False         # لأن preprocess تقوم بتحويل الحروف إلى صغيرة بالفعل\n",
    ")\n",
    "\n",
    "# 4. تطبيق TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. حفظ النتائج\n",
    "output_dir = r\"..\\data\\beir_quora_test\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_beir_quora_test.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_beir_quora_test.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_beir_quora_test.joblib\"))\n",
    "\n",
    "print(\"✅ تم حفظ ملفات TF-IDF بنجاح.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dc497-2ae9-4bb8-99ea-5ae32c225f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
