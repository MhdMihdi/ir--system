{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d37e3f-a8c1-4612-add5-b98dde8e3f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 380000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 390000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 400000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 410000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 420000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 430000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 440000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 450000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 460000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 470000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 480000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 490000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 500000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "\n",
      "ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...\n",
      "\n",
      "ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\n",
      "..\\data\\msmarco_train\\processed\\processed_msmarco_train.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø­Ù…Ù‘Ù„Ø©\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "# 2. Ù‚Ø±Ø§Ø¡Ø© 200 Ø£Ù„Ù ÙˆØ«ÙŠÙ‚Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 500_000\n",
    "seen_ids = set()  # Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ø¹Ø±Ù‘ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ…Øª Ø±Ø¤ÙŠØªÙ‡Ø§\n",
    "\n",
    "# 3. Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø´ÙˆÙ‡Ø©\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    # ÙÙƒ ØªØ±Ù…ÙŠØ² HTML Ù…Ø«Ù„ &amp; Ùˆ &quot;\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©\n",
    "    text = text.lower()\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…ÙØ±Ø¯\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© (Ù…Ø«Ù„ cooool â†’ cool)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 5. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 7. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©\n",
    "print(\"\\nğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...\")\n",
    "df['clean_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 8. Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© (ÙƒØ¥Ø¬Ø±Ø§Ø¡ Ø¥Ø¶Ø§ÙÙŠ)\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 9. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "output_dir = r\"..\\data\\msmarco_train\\processed\"\n",
    "output_file = os.path.join(output_dir, \"processed_msmarco_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'clean_text']].to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce83516-0d19-43b8-966c-91d4599037b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "input_file = r\"C:\\Users\\Mohammad Mihdi\\Desktop\\Projects\\IR\\data\\msmarco_train\\processed\\processed_msmarco_train.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ§Øª\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [doc['clean_text'] for doc in data]\n",
    "\n",
    "# 3. Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ù…Ø¹ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "\n",
    "import html\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=advanced_preprocess,\n",
    "    tokenizer=str.split,\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# 4. ØªØ·Ø¨ÙŠÙ‚ TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "output_dir = r\"C:\\Users\\Mohammad Mihdi\\Desktop\\Projects\\IR\\data\\msmarco_train\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_msmarco_train.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_msmarco_train.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_msmarco_train.joblib\"))\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e986394e-ed85-4861-84cd-08bdbbbf3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohammad\n",
      "[nltk_data]     Mihdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\n",
      "\n",
      "ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...\n",
      "\n",
      "ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\n",
      "..\\data\\antique_train\\processed\\processed_antique_train.json\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø­Ù…Ù‘Ù„Ø©\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "# 2. Ù‚Ø±Ø§Ø¡Ø© 200 Ø£Ù„Ù ÙˆØ«ÙŠÙ‚Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³\n",
    "docs_data = []\n",
    "count = 0\n",
    "LIMIT = 405_000\n",
    "seen_ids = set()  # Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ø¹Ø±Ù‘ÙØ§Øª Ø§Ù„ØªÙŠ ØªÙ…Øª Ø±Ø¤ÙŠØªÙ‡Ø§\n",
    "\n",
    "# 3. Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø´ÙˆÙ‡Ø©\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªØ±Ø§ÙÙŠ\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    # ÙÙƒ ØªØ±Ù…ÙŠØ² HTML Ù…Ø«Ù„ &amp; Ùˆ &quot;\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©\n",
    "    text = text.lower()\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…ÙØ±Ø¯\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© (Ù…Ø«Ù„ cooool â†’ cool)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 5. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§\n",
    "# 5. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØªÙ†Ø¸ÙŠÙÙ‡Ø§\n",
    "doc_iterator = dataset.docs_iter()\n",
    "while count < LIMIT:\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "    except StopIteration:\n",
    "        print(\"ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue  # ØªØ¬Ø§ÙˆØ² Ø£ÙŠ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹\n",
    "\n",
    "    if doc.doc_id in seen_ids:\n",
    "        continue\n",
    "\n",
    "    seen_ids.add(doc.doc_id)\n",
    "    text_clean = safe_text(doc.text)\n",
    "    docs_data.append({\n",
    "        \"id\": doc.doc_id,\n",
    "        \"text\": text_clean\n",
    "    })\n",
    "    count += 1\n",
    "\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "\n",
    "\n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "# 7. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©\n",
    "print(\"\\nğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...\")\n",
    "df['clean_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# 8. Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© (ÙƒØ¥Ø¬Ø±Ø§Ø¡ Ø¥Ø¶Ø§ÙÙŠ)\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# 9. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "output_dir = r\"..\\data\\antique_train\\processed\"\n",
    "output_file = os.path.join(output_dir, \"processed_antique_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df[['id', 'clean_text']].to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ù„Ù:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4feb8fb-a6ad-4723-a4de-bc0cbf0a00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "input_file = r\"..\\data\\antique_train\\processed\\processed_antique_train.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ§Øª\n",
    "doc_ids = [doc['id'] for doc in data]\n",
    "texts = [doc['clean_text'] for doc in data]\n",
    "\n",
    "# 3. Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ù…Ø¹ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "\n",
    "import html\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=advanced_preprocess,\n",
    "    tokenizer=str.split,\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# 4. ØªØ·Ø¨ÙŠÙ‚ TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "output_dir = r\"..\\data\\antique_train\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_antique_train.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_antique_train.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_antique_train.joblib\"))\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a16278-5306-4aba-9587-9bf2dfb68974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from services.documents_service import preprocess\n",
    "\n",
    "# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "input_file = r\"..\\data\\beir_quora_test\\processed\\processed_beir_quora_test.json\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ§Øª\n",
    "doc_ids = [doc['doc_id'] for doc in data]\n",
    "texts = [\" \".join(doc['tokens']) for doc in data]  # Ù†ØµÙˆØµ Ù†ØµÙŠØ©\n",
    "\n",
    "# 3. Ø¥Ø¹Ø¯Ø§Ø¯ TfidfVectorizer Ù…Ø¹ preprocess ÙƒÙ€ tokenizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocess,   # Ø¯Ø§Ù„Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
    "    preprocessor=None,      # Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… preprocessor Ø¢Ø®Ø±\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    lowercase=False         # Ù„Ø£Ù† preprocess ØªÙ‚ÙˆÙ… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø¥Ù„Ù‰ ØµØºÙŠØ±Ø© Ø¨Ø§Ù„ÙØ¹Ù„\n",
    ")\n",
    "\n",
    "# 4. ØªØ·Ø¨ÙŠÙ‚ TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 5. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "output_dir = r\"..\\data\\beir_quora_test\\index\\TFIDF\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(output_dir, \"tfidf_vectorizer_beir_quora_test.joblib\"))\n",
    "joblib.dump(tfidf_matrix, os.path.join(output_dir, \"tfidf_matrix_beir_quora_test.joblib\"))\n",
    "joblib.dump(doc_ids, os.path.join(output_dir, \"doc_ids_beir_quora_test.joblib\"))\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„ÙØ§Øª TF-IDF Ø¨Ù†Ø¬Ø§Ø­.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dc497-2ae9-4bb8-99ea-5ae32c225f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
