{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36de3b2-05a3-46fd-8472-9cf7e6c4d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 380000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 390000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 400000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 410000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 420000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 430000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 440000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 450000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 460000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 470000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 480000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 490000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 500000 ÙˆØ«ÙŠÙ‚Ø©...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "\n",
    "seen_ids = set()\n",
    "\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "count = 0\n",
    "\n",
    "docs_data = []\n",
    "\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "while count < 500000:\n",
    "\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "df.drop(df.loc[df['id']==''].index,inplace=True)\n",
    "\n",
    "df['id']=df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c05ee9-4c50-4bd1-a80b-a59067164401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"..\\data\\msmarco_train\\raw\"\n",
    "output_file = os.path.join(output_dir, \"raw_msmarco_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_json(output_file, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691d2200-a5fd-4829-9979-2e6ad0067a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "seen_ids = set()\n",
    "\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "count = 0\n",
    "\n",
    "docs_data = []\n",
    "\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "while count < 404000:\n",
    "\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "            \n",
    "    except StopIteration:\n",
    "        print(\"ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue  # ØªØ¬Ø§ÙˆØ² Ø£ÙŠ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ \n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "df.drop(df.loc[df['id']==''].index,inplace=True)\n",
    "\n",
    "df['id']=df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30f1903-ab76-44d4-8e0c-31da9504f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"..\\data\\antique_train\\raw\"\n",
    "output_file = os.path.join(output_dir, \"raw_antique_train.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_json(output_file, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea47923-2b1e-493b-92d2-27c71f7a9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 10000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 20000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 30000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 40000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 50000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 60000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 70000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 80000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 90000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 100000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 110000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 120000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 130000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 140000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 160000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 170000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 180000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 190000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 200000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 210000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 220000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 230000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 240000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 250000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 260000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 270000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 280000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 290000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 300000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 310000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 320000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 330000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 340000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 350000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 360000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 370000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 380000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 390000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 400000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 410000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 420000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 430000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 440000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 450000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 460000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 470000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 480000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 490000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 500000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 510000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 520000 ÙˆØ«ÙŠÙ‚Ø©...\n",
      "ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load(\"beir/quora/test\")\n",
    "\n",
    "seen_ids = set()\n",
    "\n",
    "doc_iterator = dataset.docs_iter()\n",
    "\n",
    "count = 0\n",
    "\n",
    "docs_data = []\n",
    "\n",
    "def safe_text(text):\n",
    "    try:\n",
    "        return text.encode('latin1').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "while count < 524000:\n",
    "\n",
    "    try:\n",
    "        doc = next(doc_iterator)\n",
    "\n",
    "        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "        if doc.doc_id in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(doc.doc_id)\n",
    "\n",
    "        text_clean = safe_text(doc.text)\n",
    "\n",
    "        docs_data.append({\n",
    "            \"id\": doc.doc_id,\n",
    "            \"text\": text_clean\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} ÙˆØ«ÙŠÙ‚Ø©...\")\n",
    "            \n",
    "    except StopIteration:\n",
    "        print(\"ğŸ“Œ Ø§Ù†ØªÙ‡Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª.\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue  # ØªØ¬Ø§ÙˆØ² Ø£ÙŠ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ \n",
    "# 6. Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
    "df = pd.DataFrame(docs_data)\n",
    "\n",
    "df.drop(df.loc[df['id']==''].index,inplace=True)\n",
    "\n",
    "df['id']=df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e06564-700d-433f-9127-69dc8280babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"..\\data\\beir\\raw\"\n",
    "output_file = os.path.join(output_dir, \"raw_beir_quora_test.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_json(output_file, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78239d0-767b-41a3-969a-e8f0611784ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Top 10 TF-IDF Results:\n",
      "Doc ID: 738009, Score: 0.4979\n",
      "Text: bacterioplankton /bakÃ‹Å’tÃ‰ÂªÃ‰â„¢rÃ‰ÂªÃ‰â„¢(ÃŠÅ )Ã‹Ë†plaÃ…â€¹(k)tÃ‰â„¢n/ /bakÃ‹Ë†tÃ‰ÂªÃ‰â„¢rÃ‰ÂªÃ‰â„¢(ÃŠÅ )Ã‹Å’plaÃ…â€¹(k)tÃ‰â„¢n/\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 557534, Score: 0.3997\n",
      "Text: A relatively stable level, period, or state: Mortgage rates declined, then reached a plateau. intr.v. plaÂ·teaued, plaÂ·teauÂ·ing, plaÂ·teaus. To reach a stable level; level off: The tension seemed to grow by degrees, then it plateaued (Tom Clancy).\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 843141, Score: 0.3858\n",
      "Text: 1. What is a Project Labor Agreement (PLA)? A project labor agreement (PLA) is a construction labor agreement between an owner and a regional building trades council, representing all the construction craft unions in a given geographical area. PLAs are agreements that establish uniform terms and conditions for all construction craft employees, as well as all construction contractors on a specific construction project. Each PLA is specific to one project only.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 843146, Score: 0.3837\n",
      "Text: A PLA is a multi-employer, multi-union, pre-hire collective bargaining agreement that PLA proponents market to public and private construction owners as a tool to systemize labor relations between multiple construction trade unions and contractors on a specific construction site.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 950385, Score: 0.3747\n",
      "Text: In 2010, PLA had the second highest consumption volume of any bioplastic of the world. The name polylactic acid does not comply with IUPAC standard nomenclature, and is potentially ambiguous or confusing, because PLA is not a polyacid (polyelectrolyte), but rather a polyester.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 348548, Score: 0.3346\n",
      "Text: Army day is observed annually on August 1. It is a day to commemorate the PLA's contribution to China and army veterans are honored on this day.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 738170, Score: 0.3344\n",
      "Text: In this lesson you will learn how to say thank you (Merci) and please (sÃ¢â‚¬â„¢il vous plait) plaÃƒÂ®t In. French you will also learn familiar and formal ways to say please as well as how to Ã¢â‚¬Ëœsay thank you veryÃ¢â‚¬â„¢. muchay please and the polite form say Ã¢â‚¬Å“sÃ¢â‚¬â„¢il vous plait Ã¢â‚¬Å“. PlaÃƒÂ®t if you are saying pleased to a, friend peer for somebody who is younger than yourself Ã¢â‚¬Å“ sayÃ¢â‚¬â„¢s il te Ã¢â‚¬Å“. Plait plaÃƒÂ®t there are several ways ofÃ¢â‚¬â„¢saying you. Re welcome the easiest and most common way of saying Ã¢â‚¬Å“ this is Ã¢â‚¬Å“. de rien\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 299331, Score: 0.3328\n",
      "Text: Share to:   The most affected organ is the heart (organ) itself. The cause of a heart attack is when the arteries that feed the heart are blocked either by a clot or build-up of pla Ã¢â‚¬Â¦ que.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 625380, Score: 0.3251\n",
      "Text: Whereas PLA is known to have low melt strength and does not show strain hardening, co-agent modification of PLA led to substantial strain hardening, suggesting improved melt strength. Improvement of melt strength and crystallization rate of polylactic acid and its blends... with medium-chain-length polyhydroxyalkanoate through reactive modification\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 953882, Score: 0.3231\n",
      "Text: One way, and the simplest, is to look on the barrel , if the weapon saysBlackPowder Only, it is not an original. Otherwise the blue book of muzzleloader values is a good plaÃ¢â‚¬Â¦ce to start. 1 person found this useful.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Œ Top 10 BERT Results:\n",
      "Doc ID: 201742, Score: 0.5182\n",
      "Text: Hey guys i am currently 15 years old and i plan to play D1 soccer. I am a sophmore whis is studying a year up and has taken 1 A.P. course. I currently hold a 3.4 and a 3.7 weighted. I play for one of the best teams in the country (Ranked top 100) out of 3,500 teams in my country.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 680939, Score: 0.4983\n",
      "Text: In the United States, the sport of association football is mainly referred to as soccer, as the term football is primarily used to refer to the sport of American football. The highest professional soccer league in the U.S. is Major League Soccer.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 702063, Score: 0.4846\n",
      "Text: He works hand in hand with two assistant referees or even three during big games. Here is a look at the soccer referee responsibilities, which are focused on ensuring a smooth running of the game. Pre-Game Responsibilities. Prior to the match, the referee must sure that the players are ready to play and all their jewelry is removed.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 149487, Score: 0.4785\n",
      "Text: The rough and tumble NFL is a league where every game counts and anything can happen on any given Sunday. When an NFL great dies, we remember their finest moments on the gridiron. This memorial site was created to honor and remember players, coaches and others associated with the National Football League.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 829492, Score: 0.4742\n",
      "Text: dropkick-drop and kick (a ball) as it touches the ground, as for a field goal. drop-kick. football, football game-any of various games played with a ball (round or oval) in which two teams try to kick or carry or propel the ball into each other's goal.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 442473, Score: 0.4727\n",
      "Text: Ã¢â‚¬â€œ Word-for-word from page 20 on U.S. SoccerÃ¢â‚¬â„¢s role with the National WomenÃ¢â‚¬â„¢s Soccer League: Ã¢â‚¬Å“National WomenÃ¢â‚¬â„¢s Soccer League, LLC (Ã¢â‚¬ËœNWSLÃ¢â‚¬â„¢) was formed on December 12, 2012 and functions as a professional womenÃ¢â‚¬â„¢s soccer league.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 695514, Score: 0.4714\n",
      "Text: Degrees. Unlike most other careers, becoming a professional football player is not dependent on the degree you pursue during college. Players who attend college on athletic scholarships with the hope of eventually making it to the NFL are occasionally known for taking light course loads.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 352601, Score: 0.4677\n",
      "Text: FA stands for Football Association in England. The FA was founded in 1863 as the governing body of the game [of soccer] in England.The FA is responsible for all regulatory a Ã¢â‚¬Â¦ spects of the game of football in England.. --From TheFA.Com.he most win fa cup team is Manchester united and they get the cup for 11 times while arsenal won the cup for 10 times, tottenham 8 times, Liverpool 7 times, Aston vila 7 time Ã¢â‚¬Â¦ s and Chelsea 6 times. 24 people found this useful.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 318124, Score: 0.4669\n",
      "Text: An intermediate goal is a goal that can be achieved in 1-5 yrs. It is one of 4 basic types of goals: short-term, intermediate, and long-term... !!!Good luck!! 3 people found this useful.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 842247, Score: 0.4660\n",
      "Text: An association football (soccer) match is presided over by a referee, whom the Laws of the Game give full authority to enforce the Laws of the Game in connection with the match to which he has been appointed (Law 5). The referee is oftentimes assisted by two assistant referees, and sometimes by a fourth official.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Œ Top 10 Hybrid Results (TF-IDF 0.4, BERT 0.6):\n",
      "Doc ID: 680939, Score: 0.3980\n",
      "Text: In the United States, the sport of association football is mainly referred to as soccer, as the term football is primarily used to refer to the sport of American football. The highest professional soccer league in the U.S. is Major League Soccer.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 674317, Score: 0.3818\n",
      "Text: Statistics and facts on the National Football League (NFL) The National Football League (NFL) is a professional football league in the United States. The league was founded in 1920 as the American Professional Football Association and played its inaugural season with eleven teams.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 738009, Score: 0.3810\n",
      "Text: bacterioplankton /bakÃ‹Å’tÃ‰ÂªÃ‰â„¢rÃ‰ÂªÃ‰â„¢(ÃŠÅ )Ã‹Ë†plaÃ…â€¹(k)tÃ‰â„¢n/ /bakÃ‹Ë†tÃ‰ÂªÃ‰â„¢rÃ‰ÂªÃ‰â„¢(ÃŠÅ )Ã‹Å’plaÃ…â€¹(k)tÃ‰â„¢n/\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 588177, Score: 0.3675\n",
      "Text: But for each of those 92 professional football players who have made it to the Super Bowl, there are approximately 11,828 students playing high school football in this country. In the 2012-2013 school year, 14,048 U.S. high schools fielded teams to play 11-man per side American tackle football.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 352602, Score: 0.3636\n",
      "Text: As the first football association, it does not use the national name English in its title. The FA is based at Wembley Stadium, London. The FA is a member of the British Olympic Association, meaning that the FA has control over the men's and women's Great Britain Olympic football team.All of England's professional football teams are members of the Football Association.he FA rejoined FIFA in 1946 and participated in their first World Cup in 1950. One of the first actions of the Football Association was to request the expulsion of the German and Japanese national football associations for their countries' role in World War II.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 913840, Score: 0.3633\n",
      "Text: Fantasy Football Glossary. New to fantasy football? Or simply want to learn all of the lingo that makes up the game? Lucky for you, NFL.com has put together a glossary of fantasy football terms so you don't spend your next season with more questions than answers.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 808484, Score: 0.3605\n",
      "Text: National Football League on television. The television rights to broadcast National Football League (NFL) games are the most lucrative and expensive rights of any American sport. Television brought professional football into prominence in the modern era after World War II.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 829492, Score: 0.3528\n",
      "Text: dropkick-drop and kick (a ball) as it touches the ground, as for a field goal. drop-kick. football, football game-any of various games played with a ball (round or oval) in which two teams try to kick or carry or propel the ball into each other's goal.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 635062, Score: 0.3520\n",
      "Text: In football you have a running back that carries the ball, and in Jugger you have a Ã¢â‚¬ËœqwikÃ¢â‚¬â„¢ that is the only one on your team that can pickup the Ã¢â‚¬ËœballÃ¢â‚¬â„¢ and score. You also have enforcers that are very much like offensive linemen on a football team.\n",
      "--------------------------------------------------------------------------------\n",
      "Doc ID: 438146, Score: 0.3516\n",
      "Text: American football. American football. Running Football Player isolated on white Stock Photo. Running Football Player isolated on white Stock Photo. Football, soccer match. Grass close up. Night event lights on the stadium. Football, soccer match. Grass close up. Night event lights on the stadium.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# ğŸ“Œ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "# =============================================\n",
    "import joblib\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“Œ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ TF-IDF\n",
    "# =============================================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª TF-IDF\n",
    "# =============================================\n",
    "tfidf_doc_ids = joblib.load(r\"..\\data\\msmarco_train\\index\\TFIDF\\doc_ids_msmarco_train.joblib\")\n",
    "tfidf_matrix = joblib.load(r\"..\\data\\msmarco_train\\index\\TFIDF\\tfidf_matrix_msmarco_train.joblib\")\n",
    "tfidf_vectorizer = joblib.load(\n",
    "    r\"..\\data\\msmarco_train\\index\\TFIDF\\tfidf_vectorizer_msmarco_train.joblib\"\n",
    ")\n",
    "inverted_index_data = joblib.load(r\"..\\data\\msmarco_train\\index\\TFIDF\\tfidf_inverted_index.joblib\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª BERT\n",
    "# =============================================\n",
    "bert_embeddings = np.load(r\"..\\data\\msmarco_train\\index\\bert\\bert_embeddings.npy\")\n",
    "bert_doc_ids = joblib.load(r\"..\\data\\msmarco_train\\index\\bert\\doc_ids.joblib\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "# =============================================\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSONL Ù„Ù„Ù†ØµÙˆØµ\n",
    "# =============================================\n",
    "docs_dict = {}\n",
    "with open(r\"..\\data\\msmarco_train\\raw\\raw_msmarco_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            doc_id = doc.get(\"id\")   \n",
    "            doc_text = doc.get(\"text\") \n",
    "            if doc_id is not None:\n",
    "                docs_dict[str(doc_id)] = doc_text\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âš ï¸ ØªØ®Ø·ÙŠ Ø³Ø·Ø± ØºÙŠØ± ØµØ§Ù„Ø­: {e}\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF ÙÙ‚Ø·\n",
    "# =============================================\n",
    "def search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_dict, top_k=10, candidate_size=100):\n",
    "    # 1. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
    "    cleaned_query = advanced_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    # 2. ØªØ±Ø´ÙŠØ­ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ (Ù…Ø¬Ù…ÙˆØ¹ Ø¯Ø±Ø¬Ø§Øª tfidf)\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index_data[\"inverted_index\"]:\n",
    "            postings = inverted_index_data[\"inverted_index\"][term]\n",
    "            for doc_id, score in postings:\n",
    "                if doc_id not in doc_scores:\n",
    "                    doc_scores[doc_id] = 0.0\n",
    "                doc_scores[doc_id] += score\n",
    "\n",
    "    # 3. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ candidate_size ÙˆØ«ÙŠÙ‚Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©\n",
    "    candidate_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:candidate_size]\n",
    "    candidate_doc_ids = [doc_id for doc_id, _ in candidate_docs]\n",
    "\n",
    "    # 4. Ø¥ÙŠØ¬Ø§Ø¯ Ø¥Ù†Ø¯ÙƒØ³Ø§Øª Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙÙŠ Ù…ØµÙÙˆÙØ© tfidf_matrix\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_index]\n",
    "\n",
    "    # 5. Ø¨Ù†Ø§Ø¡ Ù…ØµÙÙˆÙØ© tfidf Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© ÙÙ‚Ø·\n",
    "    candidate_tfidf_matrix = tfidf_matrix[candidate_indices]\n",
    "\n",
    "    # 6. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ tfidf\n",
    "    query_vector = tfidf_vectorizer.transform([cleaned_query])\n",
    "\n",
    "    # 7. Ø­Ø³Ø§Ø¨ cosine similarity Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© ÙÙ‚Ø·\n",
    "    cosine_scores = cosine_similarity(query_vector, candidate_tfidf_matrix).flatten()\n",
    "\n",
    "    # 8. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ top_k ÙˆØ«Ø§Ø¦Ù‚ Ø­Ø³Ø¨ cosine similarity\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    # 9. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¹Ø±Ø¶\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_idx = candidate_indices[idx]\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        doc_text = docs_dict.get(str(doc_id), \"âš ï¸ Ù†Øµ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.\")\n",
    "        score = cosine_scores[idx]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT ÙÙ‚Ø·\n",
    "# =============================================\n",
    "def search_bert(query, top_k=10):\n",
    "    query_embedding = bert_model.encode([query])\n",
    "    bert_scores = cosine_similarity(query_embedding, bert_embeddings).flatten()\n",
    "\n",
    "    top_indices = np.argsort(bert_scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        doc_id = bert_doc_ids[i]\n",
    "        doc_text = docs_dict.get(doc_id, \"âš ï¸ Ù†Øµ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.\")\n",
    "        score = bert_scores[i]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬ Ø¨ÙŠÙ† TF-IDF Ùˆ BERT\n",
    "# =============================================\n",
    "def search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    assert tfidf_weight + bert_weight == 1.0, \"ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† 1.0\"\n",
    "\n",
    "\n",
    "    tfidf_scores = cosine_similarity(tfidf_vectorizer.transform([advanced_preprocess(query)]), tfidf_matrix).flatten()\n",
    "    bert_scores = cosine_similarity(bert_model.encode([query]), bert_embeddings).flatten()\n",
    "\n",
    "    if tfidf_doc_ids != bert_doc_ids:\n",
    "        raise ValueError(\"Ù‚ÙˆØ§Ø¦Ù… doc_ids ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†!\")\n",
    "\n",
    "    combined_scores = tfidf_weight * tfidf_scores + bert_weight * bert_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        doc_id = tfidf_doc_ids[i]\n",
    "        doc_text = docs_dict.get(doc_id, \"âš ï¸ Ù†Øµ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.\")\n",
    "        score = combined_scores[i]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ğŸ§ª ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¨Ø­Ø«\n",
    "# =============================================\n",
    "query = \"I pla football\"\n",
    "\n",
    "# Ù†ØªØ§Ø¦Ø¬ TF-IDF ÙÙ‚Ø·\n",
    "print(\"ğŸ“Œ Top 10 TF-IDF Results:\")\n",
    "results_tfidf = search_tfidf_with_inverted_index(\n",
    "    query=query,\n",
    "    inverted_index_data=inverted_index_data, \n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    tfidf_matrix=tfidf_matrix,\n",
    "    doc_ids=tfidf_doc_ids,\n",
    "    docs_dict=docs_dict,\n",
    "    top_k=10,\n",
    "    candidate_size=100\n",
    ")\n",
    "\n",
    "for doc_id, doc_text, score in results_tfidf:\n",
    "    print(f\"Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "    print(f\"Text: {doc_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Ù†ØªØ§Ø¦Ø¬ BERT ÙÙ‚Ø·\n",
    "print(\"\\nğŸ“Œ Top 10 BERT Results:\")\n",
    "results_bert = search_bert(query)\n",
    "for doc_id, doc_text, score in results_bert:\n",
    "    print(f\"Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "    print(f\"Text: {doc_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Ù†ØªØ§Ø¦Ø¬ Hybrid (TF-IDF + BERT)\n",
    "print(\"\\nğŸ“Œ Top 10 Hybrid Results (TF-IDF 0.4, BERT 0.6):\")\n",
    "results_hybrid = search_hybrid(query, tfidf_weight=0.4, bert_weight=0.6)\n",
    "\n",
    "for doc_id, doc_text, score in results_hybrid:\n",
    "    print(f\"Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "    print(f\"Text: {doc_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
