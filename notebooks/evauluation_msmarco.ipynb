{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186d15c0-9765-4e81-a5e9-1623ec9ca007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting trectools\n",
      "  Downloading trectools-0.0.50.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=0.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (1.5.1)\n",
      "Requirement already satisfied: scipy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (1.13.1)\n",
      "Collecting sarge>=0.1.1 (from trectools)\n",
      "  Downloading sarge-0.1.7.post1-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (5.2.1)\n",
      "Collecting bs4>=0.0.0.1 (from trectools)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: matplotlib>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from trectools) (3.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4>=0.0.0.1->trectools) (4.12.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.5->trectools) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.15.0->trectools) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.15->trectools) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.5->trectools) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4>=0.0.0.1->trectools) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading sarge-0.1.7.post1-py2.py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: trectools\n",
      "  Building wheel for trectools (setup.py): started\n",
      "  Building wheel for trectools (setup.py): finished with status 'done'\n",
      "  Created wheel for trectools: filename=trectools-0.0.50-py3-none-any.whl size=28590 sha256=06f27cbf2cc8fb68ce55966f4e5f7656226b853aaca450c1ae18e909e970a3ec\n",
      "  Stored in directory: c:\\users\\mohammad mihdi\\appdata\\local\\pip\\cache\\wheels\\67\\23\\68\\7e98dcdfd7724e9b433a040c4545f11a6b8cfc083678ea9cbc\n",
      "Successfully built trectools\n",
      "Installing collected packages: sarge, bs4, trectools\n",
      "Successfully installed bs4-0.0.2 sarge-0.1.7.post1 trectools-0.0.50\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trectools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb13343-9f13-42c1-a997-cedc3beae372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
      "  Downloading editdistpy-0.1.6-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.6 MB 212.3 kB/s eta 0:00:10\n",
      "   -------- ------------------------------- 0.5/2.6 MB 212.3 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.8/2.6 MB 325.6 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 112.8 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 112.8 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 112.8 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 112.8 kB/s eta 0:00:14\n",
      "   -------------------- ------------------- 1.3/2.6 MB 134.5 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 1.6/2.6 MB 163.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.6/2.6 MB 163.5 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 189.9 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.1/2.6 MB 212.0 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.4/2.6 MB 237.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 2.4/2.6 MB 237.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 2.4/2.6 MB 237.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 2.6/2.6 MB 247.4 kB/s eta 0:00:00\n",
      "Downloading editdistpy-0.1.6-cp312-cp312-win_amd64.whl (162 kB)\n",
      "Installing collected packages: editdistpy, symspellpy\n",
      "Successfully installed editdistpy-0.1.6 symspellpy-6.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeef6f59-2e3b-4276-a95c-021f0e7c2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806a233-71a2-4831-a868-127a2576e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TFIDF: 100%|████████████████████████████████████████████████████████████| 10000/10000 [01:09<00:00, 143.88it/s]\n",
      "Running BERT: 100%|█████████████████████████████████████████████████████████████| 10000/10000 [01:34<00:00, 105.27it/s]\n",
      "Running Hybrid: 100%|███████████████████████████████████████████████████████████| 10000/10000 [01:16<00:00, 131.19it/s]\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\trectools\\trec_eval.py:311: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  selection = selection[~selection[\"rel\"].isnull()].groupby(\"query\").first().copy()\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\trectools\\trec_eval.py:311: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  selection = selection[~selection[\"rel\"].isnull()].groupby(\"query\").first().copy()\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\trectools\\trec_eval.py:311: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  selection = selection[~selection[\"rel\"].isnull()].groupby(\"query\").first().copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "╒═════════╤════════╤════════╤════════╤══════════╕\n",
      "│ Model   │    MAP │    MRR │   P@10 │   Recall │\n",
      "╞═════════╪════════╪════════╪════════╪══════════╡\n",
      "│ TFIDF   │ 0.2682 │ 0.2734 │ 0.0599 │   0.5729 │\n",
      "├─────────┼────────┼────────┼────────┼──────────┤\n",
      "│ BERT    │ 0.2644 │ 0.2702 │ 0.0567 │   0.5394 │\n",
      "├─────────┼────────┼────────┼────────┼──────────┤\n",
      "│ Hybrid  │ 0.3438 │ 0.3501 │ 0.0733 │   0.6983 │\n",
      "╘═════════╧════════╧════════╧════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 📦 المكتبات المطلوبة\n",
    "# =============================================\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from trectools import TrecQrel, TrecRun, TrecEval \n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Memory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# =============================================\n",
    "# ⚙️ تهيئة الكاش\n",
    "# =============================================\n",
    "memory = Memory(location='./cache', verbose=0)\n",
    "\n",
    "# =============================================\n",
    "# ⚙️ تحميل بيانات ir_datasets (MSMARCO)\n",
    "# =============================================\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "queries_path = os.path.expanduser(\"~/.ir_datasets/msmarco-passage/train/queries.tsv\")\n",
    "\n",
    "queries = {}\n",
    "with open(queries_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            queries[parts[0]] = parts[1]\n",
    "\n",
    "qrels = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.relevance > 0:\n",
    "        qrels.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
    "\n",
    "# =============================================\n",
    "# 🧼 دالة التنظيف\n",
    "# =============================================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# =============================================\n",
    "# تحميل ملفات التمثيلات\n",
    "# =============================================\n",
    "tfidf_doc_ids = joblib.load(r\"../data/msmarco_train/index/TFIDF/doc_ids_msmarco_train.joblib\")\n",
    "tfidf_matrix = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_matrix_msmarco_train.joblib\")\n",
    "tfidf_vectorizer = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_vectorizer_msmarco_train.joblib\")\n",
    "inverted_index_data = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_inverted_index.joblib\")\n",
    "\n",
    "bert_embeddings = np.load(r\"../data/msmarco_train/index/bert/bert_embeddings.npy\")\n",
    "bert_doc_ids = joblib.load(r\"../data/msmarco_train/index/bert/doc_ids.joblib\")\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "docs_dict = {}\n",
    "with open(r\"../data/msmarco_train/raw/raw_msmarco_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            j = json.loads(line)\n",
    "            docs_dict[str(j[\"id\"])] = j[\"text\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# =============================================\n",
    "# ✳️ فلترة qrels و queries\n",
    "# =============================================\n",
    "available_doc_ids = set(docs_dict.keys())\n",
    "filtered_qrels = {\n",
    "    qid: {docid for docid in docids if docid in available_doc_ids}\n",
    "    for qid, docids in qrels.items()\n",
    "}\n",
    "filtered_qrels = {qid: docids for qid, docids in filtered_qrels.items() if docids}\n",
    "filtered_queries = {qid: queries[qid] for qid in filtered_qrels}\n",
    "\n",
    "qrels = filtered_qrels\n",
    "queries = filtered_queries\n",
    "\n",
    "# أخذ أول 5000 استعلام فقط\n",
    "sample_queries = dict(list(queries.items())[:10000])\n",
    "\n",
    "# =============================================\n",
    "# 🔍 دوال البحث الأصلية\n",
    "# =============================================\n",
    "def search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_dict, top_k=10, candidate_size=100):\n",
    "    cleaned_query = advanced_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index_data[\"inverted_index\"]:\n",
    "            postings = inverted_index_data[\"inverted_index\"][term]\n",
    "            for doc_id, score in postings:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score\n",
    "\n",
    "    candidate_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:candidate_size]\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id, _ in candidate_docs if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "\n",
    "    candidate_tfidf_matrix = tfidf_matrix[candidate_indices]\n",
    "    query_vector = tfidf_vectorizer.transform([cleaned_query])\n",
    "    cosine_scores = cosine_similarity(query_vector, candidate_tfidf_matrix).flatten()\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_idx = candidate_indices[idx]\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        doc_text = docs_dict.get(doc_id, \"\")\n",
    "        score = cosine_scores[idx]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "def search_bert(query, top_k=10):\n",
    "    query_embedding = bert_model.encode([query])\n",
    "    bert_scores = cosine_similarity(query_embedding, bert_embeddings).flatten()\n",
    "    top_indices = np.argsort(bert_scores)[::-1][:top_k]\n",
    "    results = [(bert_doc_ids[i], docs_dict.get(bert_doc_ids[i], \"\"), bert_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "def search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    tfidf_scores = cosine_similarity(tfidf_vectorizer.transform([advanced_preprocess(query)]), tfidf_matrix).flatten()\n",
    "    bert_scores = cosine_similarity(bert_model.encode([query]), bert_embeddings).flatten()\n",
    "    combined_scores = tfidf_weight * tfidf_scores + bert_weight * bert_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    results = [(tfidf_doc_ids[i], docs_dict.get(tfidf_doc_ids[i], \"\"), combined_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# =============================================\n",
    "# 🧠 تغليف بالكاش\n",
    "# =============================================\n",
    "@memory.cache\n",
    "def cached_search_tfidf(query, top_k=10, candidate_size=100):\n",
    "    return search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, tfidf_doc_ids, docs_dict, top_k, candidate_size)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_bert(query, top_k=10):\n",
    "    return search_bert(query, top_k)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    return search_hybrid(query, tfidf_weight, bert_weight, top_k)\n",
    "\n",
    "# =============================================\n",
    "# 📁 دوال كتابة run و qrel\n",
    "# =============================================\n",
    "def write_qrel_file(qrels, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f\"{qid} 0 {docid} 1\\n\")\n",
    "\n",
    "def write_run_file_threaded(search_fn, queries, run_name, filepath, top_k=10, max_workers=8):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(search_fn, query, top_k=top_k): qid\n",
    "                for qid, query in queries.items()\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Running {run_name}\"):\n",
    "                qid = futures[future]\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    for rank, (doc_id, _, score) in enumerate(results, start=1):\n",
    "                        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in query {qid}: {e}\")\n",
    "\n",
    "# =============================================\n",
    "# 📈 التقييم\n",
    "# =============================================\n",
    "qrel_path = \"filtered_msmarco.qrel\"\n",
    "run_tfidf_path = \"run_tfidf.txt\"\n",
    "run_bert_path = \"run_bert.txt\"\n",
    "run_hybrid_path = \"run_hybrid.txt\"\n",
    "\n",
    "write_qrel_file(qrels, qrel_path)\n",
    "write_run_file_threaded(cached_search_tfidf, sample_queries, \"TFIDF\", run_tfidf_path, top_k=10)\n",
    "write_run_file_threaded(cached_search_bert, sample_queries, \"BERT\", run_bert_path, top_k=10)\n",
    "write_run_file_threaded(lambda q, top_k=10: cached_search_hybrid(q, tfidf_weight=0.4, bert_weight=0.6, top_k=top_k), sample_queries, \"Hybrid\", run_hybrid_path, top_k=10)\n",
    "\n",
    "qrel = TrecQrel(qrel_path)\n",
    "runs = {\n",
    "    \"TFIDF\": TrecRun(run_tfidf_path),\n",
    "    \"BERT\": TrecRun(run_bert_path),\n",
    "    \"Hybrid\": TrecRun(run_hybrid_path),\n",
    "}\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, run in runs.items():\n",
    "    evaluation = TrecEval(run, qrel)\n",
    "    model_results = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAP\": evaluation.get_map(),\n",
    "        \"MRR\": evaluation.get_reciprocal_rank(),\n",
    "        \"P@10\": evaluation.get_precision(10),\n",
    "        \"Recall\": evaluation.get_recall(1000)\n",
    "    }\n",
    "    results_table.append(model_results)\n",
    "\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "print(tabulate(results_table, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".4f\"))\n",
    "\n",
    "for path in [qrel_path, run_tfidf_path, run_bert_path, run_hybrid_path]:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except OSError as e:\n",
    "        print(f\"⚠️ فشل حذف {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a69c5-764d-49d5-99ee-f99b8ffc4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 📦 المكتبات المطلوبة\n",
    "# =============================================\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from trectools import TrecQrel, TrecRun, TrecEval \n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Memory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from services.vector_store import VectorStore \n",
    "# =============================================\n",
    "# ⚙️ تهيئة الكاش\n",
    "# =============================================\n",
    "memory = Memory(location='./cache', verbose=0)\n",
    "\n",
    "# =============================================\n",
    "# ⚙️ تحميل بيانات ir_datasets (MSMARCO)\n",
    "# =============================================\n",
    "dataset = ir_datasets.load(\"msmarco-passage/train\")\n",
    "queries_path = os.path.expanduser(\"~/.ir_datasets/msmarco-passage/train/queries.tsv\")\n",
    "\n",
    "queries = {}\n",
    "with open(queries_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            queries[parts[0]] = parts[1]\n",
    "\n",
    "qrels = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.relevance > 0:\n",
    "        qrels.setdefault(qrel.query_id, set()).add(qrel.doc_id)\n",
    "\n",
    "# =============================================\n",
    "# 🧼 دالة التنظيف\n",
    "# =============================================\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    text = html.unescape(text)\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [stemmer.stem(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# =============================================\n",
    "# تحميل ملفات التمثيلات\n",
    "# =============================================\n",
    "tfidf_doc_ids = joblib.load(r\"../data/msmarco_train/index/TFIDF/doc_ids_msmarco_train.joblib\")\n",
    "tfidf_matrix = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_matrix_msmarco_train.joblib\")\n",
    "tfidf_vectorizer = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_vectorizer_msmarco_train.joblib\")\n",
    "inverted_index_data = joblib.load(r\"../data/msmarco_train/index/TFIDF/tfidf_inverted_index.joblib\")\n",
    "\n",
    "bert_embeddings = np.load(r\"../data/msmarco_train/index/bert/bert_embeddings.npy\")\n",
    "bert_doc_ids = joblib.load(r\"../data/msmarco_train/index/bert/doc_ids.joblib\")\n",
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store_path = r\"../data/msmarco_train/index/vector_store\"\n",
    "vector_store = VectorStore.load(vector_store_path)\n",
    "\n",
    "docs_dict = {}\n",
    "with open(r\"../data/msmarco_train/raw/raw_msmarco_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            j = json.loads(line)\n",
    "            docs_dict[str(j[\"id\"])] = j[\"text\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# =============================================\n",
    "# ✳️ فلترة qrels و queries\n",
    "# =============================================\n",
    "available_doc_ids = set(docs_dict.keys())\n",
    "filtered_qrels = {\n",
    "    qid: {docid for docid in docids if docid in available_doc_ids}\n",
    "    for qid, docids in qrels.items()\n",
    "}\n",
    "filtered_qrels = {qid: docids for qid, docids in filtered_qrels.items() if docids}\n",
    "filtered_queries = {qid: queries[qid] for qid in filtered_qrels}\n",
    "\n",
    "qrels = filtered_qrels\n",
    "queries = filtered_queries\n",
    "\n",
    "# أخذ أول 5000 استعلام فقط\n",
    "sample_queries = dict(list(queries.items())[:10000])\n",
    "\n",
    "# =============================================\n",
    "# 🔍 دوال البحث الأصلية\n",
    "# =============================================\n",
    "def search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_dict, top_k=10, candidate_size=100):\n",
    "    cleaned_query = advanced_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "    if not query_terms:\n",
    "        return []\n",
    "\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index_data[\"inverted_index\"]:\n",
    "            postings = inverted_index_data[\"inverted_index\"][term]\n",
    "            for doc_id, score in postings:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score\n",
    "\n",
    "    candidate_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:candidate_size]\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "    candidate_indices = [doc_id_to_index[doc_id] for doc_id, _ in candidate_docs if doc_id in doc_id_to_index]\n",
    "\n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "\n",
    "    candidate_tfidf_matrix = tfidf_matrix[candidate_indices]\n",
    "    query_vector = tfidf_vectorizer.transform([cleaned_query])\n",
    "    cosine_scores = cosine_similarity(query_vector, candidate_tfidf_matrix).flatten()\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        doc_idx = candidate_indices[idx]\n",
    "        doc_id = doc_ids[doc_idx]\n",
    "        doc_text = docs_dict.get(doc_id, \"\")\n",
    "        score = cosine_scores[idx]\n",
    "        results.append((doc_id, doc_text, score))\n",
    "    return results\n",
    "\n",
    "def search_bert(query, top_k=10):\n",
    "     query_embedding = bert_model.encode([query],normalize_embeddings=True).astype(np.float32)\n",
    "     return vector_store.search(query_embedding, top_k=top_k)\n",
    "\n",
    "def search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    tfidf_scores = cosine_similarity(tfidf_vectorizer.transform([advanced_preprocess(query)]), tfidf_matrix).flatten()\n",
    "    query_embedding = bert_model.encode([query], normalize_embeddings=True).astype(np.float32)\n",
    "    # Initialize empty BERT scores\n",
    "    bert_scores = np.zeros_like(tfidf_scores)\n",
    "    top_bert_results = vector_store.search(query_embedding, top_k=top_k * 20)\n",
    "        # أنشئ mapping أسرع\n",
    "    doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(bert_doc_ids)}\n",
    "\n",
    "    for doc_id, _, score in top_bert_results:\n",
    "        idx = doc_id_to_index.get(doc_id)\n",
    "        if idx is not None:\n",
    "           bert_scores[idx] = score\n",
    "            \n",
    "    if tfidf_doc_ids != bert_doc_ids:\n",
    "        raise ValueError(\"قوائم doc_ids غير متطابقة بين النموذجين!\")\n",
    "        \n",
    "    combined_scores = tfidf_weight * tfidf_scores + bert_weight * bert_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    results = [(tfidf_doc_ids[i], docs_dict.get(tfidf_doc_ids[i], \"\"), combined_scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# =============================================\n",
    "# 🧠 تغليف بالكاش\n",
    "# =============================================\n",
    "@memory.cache\n",
    "def cached_search_tfidf(query, top_k=10, candidate_size=100):\n",
    "    return search_tfidf_with_inverted_index(query, inverted_index_data, tfidf_vectorizer, tfidf_matrix, tfidf_doc_ids, docs_dict, top_k, candidate_size)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_bert(query, top_k=10):\n",
    "    return search_bert(query, top_k)\n",
    "\n",
    "@memory.cache\n",
    "def cached_search_hybrid(query, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    return search_hybrid(query, tfidf_weight, bert_weight, top_k)\n",
    "\n",
    "# =============================================\n",
    "# 📁 دوال كتابة run و qrel\n",
    "# =============================================\n",
    "def write_qrel_file(qrels, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f\"{qid} 0 {docid} 1\\n\")\n",
    "\n",
    "def write_run_file_threaded(search_fn, queries, run_name, filepath, top_k=10, max_workers=8):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(search_fn, query, top_k=top_k): qid\n",
    "                for qid, query in queries.items()\n",
    "            }\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Running {run_name}\"):\n",
    "                qid = futures[future]\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    for rank, (doc_id, _, score) in enumerate(results, start=1):\n",
    "                        f.write(f\"{qid} Q0 {doc_id} {rank} {score} {run_name}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error in query {qid}: {e}\")\n",
    "\n",
    "# =============================================\n",
    "# 📈 التقييم\n",
    "# =============================================\n",
    "qrel_path = \"filtered_msmarco.qrel\"\n",
    "run_tfidf_path = \"run_tfidf.txt\"\n",
    "run_bert_path = \"run_bert.txt\"\n",
    "run_hybrid_path = \"run_hybrid.txt\"\n",
    "\n",
    "write_qrel_file(qrels, qrel_path)\n",
    "write_run_file_threaded(cached_search_tfidf, sample_queries, \"TFIDF\", run_tfidf_path, top_k=10)\n",
    "write_run_file_threaded(cached_search_bert, sample_queries, \"BERT\", run_bert_path, top_k=10)\n",
    "write_run_file_threaded(lambda q, top_k=10: cached_search_hybrid(q, tfidf_weight=0.4, bert_weight=0.6, top_k=top_k), sample_queries, \"Hybrid\", run_hybrid_path, top_k=10)\n",
    "\n",
    "qrel = TrecQrel(qrel_path)\n",
    "runs = {\n",
    "    \"TFIDF\": TrecRun(run_tfidf_path),\n",
    "    \"BERT\": TrecRun(run_bert_path),\n",
    "    \"Hybrid\": TrecRun(run_hybrid_path),\n",
    "}\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, run in runs.items():\n",
    "    evaluation = TrecEval(run, qrel)\n",
    "    model_results = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAP\": evaluation.get_map(),\n",
    "        \"MRR\": evaluation.get_reciprocal_rank(),\n",
    "        \"P@10\": evaluation.get_precision(10),\n",
    "        \"Recall\": evaluation.get_recall(1000)\n",
    "    }\n",
    "    results_table.append(model_results)\n",
    "\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "print(tabulate(results_table, headers=\"keys\", tablefmt=\"fancy_grid\", floatfmt=\".4f\"))\n",
    "\n",
    "for path in [qrel_path, run_tfidf_path, run_bert_path, run_hybrid_path]:\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except OSError as e:\n",
    "        print(f\"⚠️ فشل حذف {path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
